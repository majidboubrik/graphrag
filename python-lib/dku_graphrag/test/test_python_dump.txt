================== ../test/query_dataiku_embedding_llm.py ==================
import asyncio
import logging
from typing import Any

import dataiku
from graphrag.query.llm.base import BaseTextEmbedding

embedding_logger = logging.getLogger("QueryDataikuEmbeddingLLM")
embedding_logger.setLevel(logging.DEBUG)
embedding_handler = logging.FileHandler("/tmp/graphrag/query_dataiku_embedding_llm.log")
embedding_handler.setLevel(logging.DEBUG)
embedding_logger.addHandler(embedding_handler)

class QueryDataikuEmbeddingLLM(BaseTextEmbedding):
    def __init__(self, project_key: str, embedding_model_id: str):
        self.project_key = project_key
        self.embedding_model_id = embedding_model_id
        self.client = dataiku.api_client()
        self.project = self.client.get_project(self.project_key)
        self.llm = self.project.get_llm(self.embedding_model_id)

    def embed(self, text: str, **kwargs: Any) -> list[float]:
        # Assuming the Dataiku LLM embedding endpoint:
        # If not available, you'll need to handle differently.
        # The hypothetical `self.llm.compute_embedding(text)` method returns an embedding vector (list of floats).
        embedding = self.llm.compute_embedding(text)
        return embedding

    async def aembed(self, text: str, **kwargs: Any) -> list[float]:
        loop = asyncio.get_running_loop()
        return await loop.run_in_executor(None, self.embed, text, **kwargs)
================== ../test/monkey_patch_dataiku.py ==================
# monkey_patch_all_dataiku.py
from dataiku_chat_llm import DataikuChatLLM
from dataiku_embeddings_llm import DataikuEmbeddingsLLM
from graphrag.config.enums import LLMType
from graphrag.index.llm.load_llm import loaders




def _load_dataiku_chat_llm(on_error, cache, config):

    project_key = "AGENTSANDBOX"
    llm_id = "openai:bs-openai:gpt-4o-mini"
    return DataikuChatLLM(project_key, llm_id)

def _load_dataiku_embeddings_llm(on_error, cache, config):
    project_key = "AGENTSANDBOX"
    embedding_model_id = "openai:bs-openai:text-embedding-3-small"
    return DataikuEmbeddingsLLM(project_key, embedding_model_id)

# Override OpenAIChat loader
loaders[LLMType.OpenAIChat] = {
    "load": _load_dataiku_chat_llm,
    "chat": True,
}

# Override the azure_openai_embedding entry to load our embeddings LLM
loaders[LLMType.AzureOpenAIEmbedding] = {
    "load": _load_dataiku_embeddings_llm,
    "chat": False,
}

print("  =============== Monkey patching loaders DONE")
================== ../test/query_monkey_patch_dataiku.py ==================
from graphrag.query.llm.get_client import get_llm, get_text_embedder
from graphrag.query.llm.base import BaseLLM, BaseTextEmbedding
from query_dataiku_chat_llm import QueryDataikuChatLLM
from query_dataiku_embedding_llm import QueryDataikuEmbeddingLLM

import graphrag.query.llm.get_client

def monkeypatched_get_llm(config):
    project_key = "AGENTSANDBOX"
    llm_id = "openai:bs-openai:gpt-4o-mini"
    return QueryDataikuChatLLM(project_key, llm_id)

def monkeypatched_get_text_embedder(config):
    project_key = "AGENTSANDBOX"
    embedding_model_id = "openai:bs-openai:text-embedding-3-small"
    return QueryDataikuEmbeddingLLM(project_key, embedding_model_id)

graphrag.query.llm.get_client.get_llm = monkeypatched_get_llm
graphrag.query.llm.get_client.get_text_embedder = monkeypatched_get_text_embedder

print("  =============== Monkey patching query LLMs DONE")
================== ../test/query.py ==================
import query_monkey_patch_dataiku 

import asyncio
import pandas as pd

from graphrag.config.load_config import load_config
from graphrag.api.query import global_search

from pathlib import Path


async def main():
    # Load configuration the same way the CLI does
    root_dir = Path("./")
    config = load_config(root_dir)
    nodes = pd.read_parquet("output/create_final_nodes.parquet")
    entities = pd.read_parquet("output/create_final_entities.parquet")
    communities = pd.read_parquet("output/create_final_communities.parquet")
    community_reports = pd.read_parquet("output/create_final_community_reports.parquet")

    # Now global_search will use the monkeypatched functions and thus DataikuChatLLM
    response, context = await global_search(
        config=config,
        nodes=nodes,
        entities=entities,
        communities=communities,
        community_reports=community_reports,
        community_level=None,
        dynamic_community_selection=False,
        response_type="multiple paragraphs",
        query="Summarize the studies please"
    )

    print("Response:", response)
    #print("Context:", context)

if __name__ == "__main__":
    asyncio.run(main())
================== ../test/test.py ==================

import monkey_patch_dataiku 

import asyncio
import logging
import time
from pathlib import Path

from datashaper.workflow.workflow import WorkflowCallbacks, WorkflowCallbacksManager
from datashaper import VerbCallbacks
from datashaper import NoopVerbCallbacks

from graphrag.api import build_index
from graphrag.callbacks.factory import create_pipeline_reporter
from graphrag.config.load_config import load_config
from graphrag.config.logging import enable_logging_with_config
from graphrag.logger.base import ProgressLogger
from graphrag.logger.factory import LoggerFactory, LoggerType
from graphrag.callbacks.progress_workflow_callbacks import ProgressWorkflowCallbacks

# A helper to wrap logging calls so they can go to both python logger and progress logger if verbose
def _logger(logger: ProgressLogger):
    def info(msg: str, verbose: bool = False):
        logging.info(msg)
        if verbose:
            logger.info(msg)

    def error(msg: str, verbose: bool = False):
        logging.error(msg)
        if verbose:
            logger.error(msg)

    def success(msg: str, verbose: bool = False):
        logging.info(msg)
        if verbose:
            logger.success(msg)

    return info, error, success

class MyCallbacks(WorkflowCallbacks):
    """Custom callbacks to print messages and handle errors."""

    def __init__(self):
        # If you want to store references to internal callbacks, you can, but it's optional.
        pass

    def on_error(self, message: str, error: Exception | None, stack: str | None, details: dict | None = None):
        logging.error(f"ERROR: {message}")
        if error:
            logging.error(f"Exception: {error}")
        if stack:
            logging.error(f"Stack Trace: {stack}")

    def on_progress(self, message: str):
        # This message is progress info: just log it as INFO
        logging.info(f"PROGRESS: {message}")

    def on_finished(self, result: dict):
        logging.info(f"FINISHED: {result}")

async def main():
    # Path to your project root containing `settings.yaml`
    root_dir = Path("./")

    # Load the configuration
    config = load_config(root_dir)

    # Set verbose to True if you want more detail
    verbose = True

    # Enable logging according to config, and control verbosity
    enable_logging_with_config(config, verbose=verbose)

    # Create a Graphrag progress logger (Rich-based)
    progress_logger = LoggerFactory().create_logger(LoggerType.RICH)

    # Wrap Python logger and progress logger
    info, error, success = _logger(progress_logger)

    # Create a WorkflowCallbacksManager to combine multiple callbacks
    manager = WorkflowCallbacksManager()

    # Add your custom callback
    my_callbacks = MyCallbacks()
    manager.register(my_callbacks)

    # Add pipeline reporter callback, just like CLI does
    reporter_callback = create_pipeline_reporter(config.reporting, None)
    manager.register(reporter_callback)

    # Add progress callbacks for nice progress output
    manager.register(ProgressWorkflowCallbacks(progress_logger))

    run_id = time.strftime("%Y%m%d-%H%M%S")
    info(f"Starting build_index with run_id={run_id}", verbose=True)

    # Now pass manager as callbacks
    outputs = await build_index(
        config=config,
        run_id=run_id,
        is_resume_run=False,
        memory_profile=False,
        progress_logger=progress_logger,
        callbacks=[manager]  # a single-element list containing the manager is OK, or pass manager directly
    )

    info("Index building completed.", verbose=True)

    # Check outputs for errors
    for output in outputs:
        if output.errors:
            for err in output.errors:
                error(f"Error in workflow {output.workflow}: {err}", verbose=True)
        else:
            success(f"Workflow completed successfully: {output.workflow}", verbose=True)

if __name__ == "__main__":
    asyncio.run(main())
================== ../test/dataiku_embeddings_llm.py ==================
# dataiku_embeddings_llm.py
import dataiku
from typing import Any, cast
from fnllm import EmbeddingsLLM, LLMOutput
from fnllm.types.generics import TJsonModel, THistoryEntry

class DataikuEmbeddingsLLM(EmbeddingsLLM):
    def __init__(self, project_key: str, embedding_model_id: str):
        self.project_key = project_key
        self.embedding_model_id = embedding_model_id
        self.client = dataiku.api_client()
        self.project = self.client.get_project(project_key)
        self.emb_model = self.project.get_llm(self.embedding_model_id)

    async def __call__(
        self,
        prompt: str,
        **kwargs
    ) -> LLMOutput[Any, TJsonModel, THistoryEntry]:
        emb_query = self.emb_model.new_embeddings()
        emb_query.add_text(prompt)
        emb_resp = emb_query.execute()
        embeddings = emb_resp.get_embeddings()[0]

        return LLMOutput(
            output={"content": embeddings},
            parsed_json=cast("TJsonModel", None),
        )

    def child(self, name):
        # Not needed for embeddings
        raise NotImplementedError
================== ../test/dataiku_chat_llm.py ==================
import json
from typing import Any, Literal
import dataiku
from fnllm import ChatLLM
from fnllm.openai.types.chat.io import (
    OpenAIChatOutput,
    OpenAIChatCompletionMessageModel,
    OpenAIChatMessageInput,
    OpenAIChatCompletionInput,
    OpenAIChatMessageInput,
)
from fnllm.openai.types.aliases import OpenAIChatCompletionUserMessageParam
from fnllm.types.generics import TJsonModel, THistoryEntry, TModelParameters
from fnllm.types.io import LLMInput, LLMOutput, JSON
from fnllm.types.metrics import LLMUsageMetrics
from fnllm.openai.types.aliases import OpenAIChatCompletionMessageModel
from typing_extensions import Unpack

import logging
# Create a logger for DataikuChatLLM
dataiku_logger = logging.getLogger("DataikuChatLLM")
dataiku_logger.setLevel(logging.DEBUG)
dataiku_handler = logging.FileHandler("/tmp/graphrag/dataiku_chat_llm.log")
dataiku_handler.setLevel(logging.DEBUG)
dataiku_logger.addHandler(dataiku_handler)


class DataikuChatLLM(ChatLLM):
    def __init__(self, project_key: str, llm_id: str):
        self.project_key = project_key
        self.llm_id = llm_id
        self.client = dataiku.api_client()
        self.project = self.client.get_project(self.project_key)
        self.llm = self.project.get_llm(self.llm_id)

    def _build_prompt_message(
        self, prompt: OpenAIChatCompletionInput
    ) -> tuple[list[OpenAIChatMessageInput], OpenAIChatMessageInput]:
        """
        If the prompt is a string, treat it as a user message.
        """
        if isinstance(prompt, str):
            prompt_message = OpenAIChatCompletionUserMessageParam(
                content=prompt,
                role="user",
            )
        else:
            # If prompt is already a structured OpenAIChatMessageInput, use it directly
            prompt_message = prompt
        return [prompt_message], prompt_message

    async def __call__(
        self,
        prompt: OpenAIChatCompletionInput,
        *,
        stream: Literal[False] | None = None,
        **kwargs: Unpack[LLMInput[TJsonModel, THistoryEntry, TModelParameters]],
    ) -> LLMOutput[OpenAIChatOutput, TJsonModel, THistoryEntry]:

        raise ValueError("tis is test")

        history = kwargs.get("history", [])
        dataiku_logger.info("DataikuChatLLM: kwargs=%s", kwargs)
        bypass_cache = kwargs.get("bypass_cache", False)

        messages, prompt_message = self._build_prompt_message(prompt)
        all_messages = [*history, *messages]

        completion = self.llm.new_completion()

        # Set the conversation messages
        for msg in all_messages:
            if isinstance(msg, OpenAIChatCompletionMessageModel):
                msg_role = "assistant"
                msg_content = msg.content
            else:
                msg_role = msg.get("role", "user")
                msg_content = msg.get("content", "")
            completion.with_message(msg_content, role=msg_role)

        # If JSON output is requested
        if kwargs.get('json', False):
            completion.with_json_output()

        resp = completion.execute()

        import re
        clean_response = re.sub(r'```json\s*|\s*```', '', resp.text)

        # Build raw_output message
        raw_output = OpenAIChatCompletionMessageModel(
            role="assistant",
            content=clean_response if resp.success else "An error occurred.",
            function_call=None,
            name=None,
            tool_calls=[],
        )

        # Create OpenAIChatOutput
        output = OpenAIChatOutput(
            raw_input=prompt_message,
            raw_output=raw_output,
            content=raw_output.content,
            usage=None,  # No usage metrics available from Dataiku LLM
        )

        llm_output = LLMOutput(
            output=output,
            # We will fill these in if json=True and json_model is provided
            raw_json=None,
            parsed_json=None,
            history=all_messages + [raw_output],
            tool_calls=[]
        )

        # If json=True and a json_model is provided, parse the JSON
        if kwargs.get("json", False) and "json_model" in kwargs:
            json_model = kwargs["json_model"]
            try:
                # Attempt to parse the cleaned response as JSON
                json_data = json.loads(clean_response)
                parsed_instance = json_model(**json_data)
                llm_output.raw_json = clean_response
                llm_output.parsed_json = parsed_instance
            except Exception as e:
                dataiku_logger.exception("Failed to parse JSON from LLM response")
                # If parsing fails, leave parsed_json as None
                llm_output.raw_json = clean_response
                llm_output.parsed_json = None

        dataiku_logger.info("DataikuChatLLM: Response text: %s", clean_response if resp.success else "Error")

        return llm_output

    def child(self, name: str):
        # Return a new instance with the same credentials
        return DataikuChatLLM(self.project_key, self.llm_id)
================== ../test/query_dataiku_chat_llm.py ==================
import asyncio
import logging
from typing import Any, Generator, AsyncGenerator, Literal

import dataiku
from graphrag.callbacks.llm_callbacks import BaseLLMCallback
from graphrag.query.llm.base import BaseLLM

dataiku_logger = logging.getLogger("QueryDataikuChatLLM")
dataiku_logger.setLevel(logging.INFO)
dataiku_handler = logging.FileHandler("/tmp/graphrag/query_dataiku_chat_llm.log")
dataiku_handler.setLevel(logging.DEBUG)
dataiku_logger.addHandler(dataiku_handler)

class QueryDataikuChatLLM(BaseLLM):
    def __init__(self, project_key: str, llm_id: str):
        self.project_key = project_key
        self.llm_id = llm_id
        self.client = dataiku.api_client()
        self.project = self.client.get_project(self.project_key)
        self.llm = self.project.get_llm(self.llm_id)

    def _prepare_completion(self, messages: str | list[Any], **kwargs: Any):
        completion = self.llm.new_completion()
        # messages can be either a string or a list of {role, content} dicts
        if isinstance(messages, str):
            # Treat as a single user message
            completion.with_message(messages, role="user")
        else:
            # Expecting a list of dicts with "role" and "content"
            for msg in messages:
                role = msg.get("role", "user")
                content = msg.get("content", "")
                completion.with_message(content, role=role)

        # If JSON mode is requested
        if kwargs.get('json', False):
            completion.with_json_output()

        return completion

    def generate(
        self,
        messages: str | list[Any],
        streaming: bool = True,
        callbacks: list[BaseLLMCallback] | None = None,
        **kwargs: Any,
    ) -> str:
        """Synchronous generation."""
        dataiku_logger.debug(f"messages: {messages}, kwargs: {kwargs}")
        completion = self._prepare_completion(messages, **kwargs)
        resp = completion.execute()
        clean_response = resp.text
        # If streaming is True but we have no streaming support, just return the full response
        return clean_response

    def stream_generate(
        self,
        messages: str | list[Any],
        callbacks: list[BaseLLMCallback] | None = None,
        **kwargs: Any,
    ) -> Generator[str, None, None]:
        """Synchronous streaming generation (if not supported, yield the entire response)."""
        # For now, just yield the entire response as one chunk
        full_response = self.generate(messages, streaming=True, callbacks=callbacks, **kwargs)
        yield full_response

    async def agenerate(
        self,
        messages: str | list[Any],
        streaming: bool = True,
        callbacks: list[BaseLLMCallback] | None = None,
        **kwargs: Any,
    ) -> str:
        """Asynchronous generation."""
        loop = asyncio.get_running_loop()

        # Create a wrapper lambda that calls self.generate with all arguments
        # Here we use keyword arguments explicitly so run_in_executor only gets a function with no extra arguments
        def sync_generate():
            return self.generate(messages=messages, streaming=streaming, callbacks=callbacks, **kwargs)

        return await loop.run_in_executor(None, sync_generate)

    async def astream_generate(
        self,
        messages: str | list[Any],
        callbacks: list[BaseLLMCallback] | None = None,
        **kwargs: Any,
    ) -> AsyncGenerator[str, None]:
        """Asynchronous streaming generation."""
        # If streaming is not truly supported, just yield the full response once
        response = await self.agenerate(messages, streaming=True, callbacks=callbacks, **kwargs)
        yield response
