================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/tools/__init__.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""Tool handling package."""

from .base import LLMTool

__all__ = ["LLMTool"]

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/tools/errors.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""Tool handling error definitions."""

from pydantic import ValidationError

from fnllm.openai.types.aliases import (
    OpenAIChatCompletionMessageModel,
    OpenAIChatCompletionMessageToolCallModel,
)

from .base import LLMTool


class ToolInvalidArgumentsError(RuntimeError):
    """Raise when a tool is called with invalid arguments."""

    def __init__(
        self,
        raw_output: OpenAIChatCompletionMessageModel,
        *,
        tool_call: OpenAIChatCompletionMessageToolCallModel,
        expected_tool: type[LLMTool],
        validation_error: ValidationError,
    ) -> None:
        """Init method definition."""
        self.raw_output = raw_output
        self.tool_call = tool_call
        self.expected_tool = expected_tool
        self.validation_error = validation_error

        super().__init__(
            f"JSON response for tool arguments does not match the expected schema, error={validation_error}."
        )


class ToolNotFoundError(RuntimeError):
    """LLM tried to call a tool that was not found."""

    def __init__(
        self,
        raw_output: OpenAIChatCompletionMessageModel,
        *,
        tool_call: OpenAIChatCompletionMessageToolCallModel,
    ) -> None:
        """Init method definition."""
        self.raw_output = raw_output
        self.tool_call = tool_call

        super().__init__(
            f"Requested tool '{tool_call.function.name}' by the LLM does not exist"
        )

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/tools/base.py ==================
# Copyright (c) 2024 Microsoft Corporation.
# Licensed under the MIT License

"""LLM base tool implementations."""

from collections.abc import Sequence
from functools import lru_cache
from typing import Any, ClassVar, cast

from pydantic import BaseModel, computed_field


class LLMTool(BaseModel):
    """Base class for tools that can be made available to the model."""

    __tool_name__: ClassVar[str | None] = None
    """Override this to replace the tool name generated from the class name by default."""

    __tool_description__: ClassVar[str | None] = None
    """Override this to replace the tool description generated from the class documentation by default."""

    __raw_arguments_json__: str | None = None
    """Raw arguments provided as a string by the model."""

    call_id: str | None = None
    """Tool call ID provided when asking for this tool to be called."""

    @computed_field
    @property
    def name(self) -> str:
        """Tool name."""
        return self.get_name()

    @computed_field
    @property
    def description(self) -> str:
        """Tool description."""
        return self.get_description()

    async def execute(self) -> Any:
        """Can be implemented to execute the tool."""
        return None

    @classmethod
    @lru_cache
    def get_name(cls) -> str:
        """Getter for the tool name (class specific)."""
        return cls.__tool_name__ or cast(str, cls.get_json_schema().get("title", ""))

    @classmethod
    @lru_cache
    def get_description(cls) -> str:
        """Getter for the tool description (class specific)."""
        return cls.__tool_description__ or cast(
            str, cls.get_json_schema().get("description", "")
        )

    @classmethod
    @lru_cache
    def get_json_schema(cls) -> dict[str, object]:
        """JSON schema for the base tool (class specific)."""
        return cls.model_json_schema()

    @classmethod
    @lru_cache
    def get_parameters_schema(cls) -> dict[str, object]:
        """Parameters schema for the base tool (class specific)."""
        schema = cls.model_json_schema()

        if "title" in schema:
            del schema["title"]

        if "description" in schema:
            del schema["description"]

        del schema["properties"]["call_id"]

        return schema

    @staticmethod
    def find_tool(
        tools: Sequence[type["LLMTool"]],
        name: str,
    ) -> type["LLMTool"] | None:
        """Find the tool that matches the given name if any. Otherwise return `None`."""
        return next(filter(lambda x: x.get_name() == name, tools), None)

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/types/metrics.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""LLM input/output types."""

from pydantic import BaseModel, Field, computed_field


class LLMUsageMetrics(BaseModel):
    """LLM request usage metrics."""

    input_tokens: int = 0
    """Used input tokens by the request."""

    output_tokens: int = 0
    """Used output tokens by the request."""

    @computed_field()
    @property
    def total_tokens(self) -> int:
        """Total tokens used by the request."""
        return self.input_tokens + self.output_tokens


class LLMRetryMetrics(BaseModel):
    """Metrics related to LLM retries."""

    num_retries: int = 0
    """Number of times the request was retried."""

    total_time: float = 0
    """Total time the request took to execute (across all retries)."""

    call_times: list[float] = Field(default_factory=list)
    """Time taken for each request try."""


class LLMMetrics(BaseModel):
    """LLM useful metrics."""

    estimated_input_tokens: int = 0
    """Estimated input tokens."""

    usage: LLMUsageMetrics = Field(default_factory=LLMUsageMetrics)
    """LLM request usage metrics."""

    retry: LLMRetryMetrics = Field(default_factory=LLMRetryMetrics)
    """LLM retry metrics."""

    @computed_field()
    @property
    def tokens_diff(self) -> int:
        """Difference between the estimated tokens and the real total token usage."""
        return self.usage.total_tokens - self.estimated_input_tokens

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/types/protocol.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""LLM protocol module."""

from typing import Any, Generic, Protocol, runtime_checkable

from typing_extensions import Unpack

from .generics import THistoryEntry, TInput, TJsonModel, TModelParameters, TOutput
from .io import LLMInput, LLMOutput


@runtime_checkable
class LLM(Protocol, Generic[TInput, TOutput, THistoryEntry, TModelParameters]):
    """LLM protocol definition."""

    async def __call__(
        self,
        prompt: TInput,
        **kwargs: Unpack[LLMInput[TJsonModel, THistoryEntry, TModelParameters]],
    ) -> LLMOutput[TOutput, TJsonModel, THistoryEntry]:  # pragma: no cover
        """Invoke the LLM, treating the LLM as a function."""
        ...

    def child(self, name: str) -> Any:
        """Create a child LLM (with child cache)."""
        ...

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/types/io.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""LLM input/output types."""

from collections.abc import Sequence
from typing import Any, Generic

from pydantic import BaseModel, Field, field_serializer
from typing_extensions import NotRequired, TypedDict

from fnllm.tools.base import LLMTool

from .generics import (
    JSON,
    PromptVariables,
    THistoryEntry,
    TJsonModel,
    TModelParameters,
    TOutput,
)
from .metrics import LLMMetrics


class LLMInput(TypedDict, Generic[TJsonModel, THistoryEntry, TModelParameters]):
    """The input of an LLM invocation."""

    name: NotRequired[str]
    """The name of the LLM invocation, if available."""

    json: NotRequired[bool]
    """If present, will attempt to elicit JSON from the LLM. Parsed JSON will be returned in the `json_output` field."""

    json_model: NotRequired[type[TJsonModel]]
    """A model to check if an LLM response is valid. Only valid if `json=True`."""

    variables: NotRequired[PromptVariables]
    """The variable replacements to use in the prompt."""

    history: NotRequired[Sequence[THistoryEntry]]
    """The history of the LLM invocation, if available (e.g. chat mode)."""

    tools: NotRequired[Sequence[type[LLMTool]]]
    """Tools to make available to the model. These are classes that implement LLMTool."""

    model_parameters: NotRequired[TModelParameters]
    """Additional model parameters to use in the LLM invocation."""

    bypass_cache: NotRequired[bool]
    """Bypass the cache (if any) for this LLM invocation."""


class LLMOutput(BaseModel, Generic[TOutput, TJsonModel, THistoryEntry]):
    """The output of an LLM invocation."""

    output: TOutput
    """The output of the LLM invocation."""

    raw_json: JSON | None = None
    """The raw JSON output from the LLM, if available."""

    parsed_json: TJsonModel | None = None
    """The parsed JSON output with a base model, if available."""

    history: list[THistoryEntry] = Field(default_factory=list)
    """The history of the LLM invocation, if available (e.g. chat mode)."""

    tool_calls: list[LLMTool] = Field(default_factory=list)
    """Tool calls required by the LLM. These will be instances of the LLM tools (with filled parameters)."""

    metrics: LLMMetrics = Field(default_factory=LLMMetrics)
    """Request/response metrics."""

    @field_serializer("tool_calls")
    def serialize_tool_calls(
        self, tool_calls: list[LLMTool]
    ) -> list[dict[str, Any]] | None:
        """Custom serialization for the tool calls (handles polymorphic types)."""
        return [tool.model_dump() for tool in tool_calls]

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/types/__init__.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""Common type definitions for the LLM package."""

from .generalized import (
    ChatLLM,
    ChatLLMInput,
    ChatLLMOutput,
    EmbeddingsLLM,
    EmbeddingsLLMInput,
    EmbeddingsLLMOutput,
)
from .generics import (
    JSON,
    PromptVariables,
    THistoryEntry,
    TInput,
    TJsonModel,
    TModelParameters,
    TOutput,
)
from .io import LLMInput, LLMOutput
from .metrics import LLMMetrics, LLMRetryMetrics, LLMUsageMetrics
from .protocol import LLM

__all__ = [
    "JSON",
    "LLM",
    "ChatLLM",
    "ChatLLMInput",
    "ChatLLMOutput",
    "EmbeddingsLLM",
    "EmbeddingsLLMInput",
    "EmbeddingsLLMOutput",
    "LLMInput",
    "LLMMetrics",
    "LLMOutput",
    "LLMRetryMetrics",
    "LLMUsageMetrics",
    "PromptVariables",
    "THistoryEntry",
    "TInput",
    "TJsonModel",
    "TModelParameters",
    "TOutput",
]

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/types/generalized.py ==================
# Copyright (c) 2024 Microsoft Corporation.
"""Generalized LLM Types."""

from typing import Any, TypeAlias, TypeVar

from pydantic import BaseModel

from .generics import THistoryEntry, TModelParameters
from .protocol import LLM

EmbeddingsLLMInput: TypeAlias = list[str] | str
"""Generic Embedding Input"""


class EmbeddingsLLMOutput(BaseModel):
    """Embedding LLM Output."""

    embeddings: list[list[float]] | None


TEmbeddingsInput = TypeVar("TEmbeddingsInput", bound=EmbeddingsLLMInput, covariant=True)
TEmbeddingsOutput = TypeVar(
    "TEmbeddingsOutput", bound=EmbeddingsLLMOutput, covariant=True
)
EmbeddingsLLM: TypeAlias = LLM[
    TEmbeddingsInput, TEmbeddingsOutput, None, TModelParameters
]
"""Embedding LLM type alias."""

ChatLLMInput: TypeAlias = str | dict[str, Any] | None
"""Generic Completion Input."""


class ChatLLMOutput(BaseModel):
    """Completion LLM Output."""

    content: str | None
    """Raw completion output."""

    def __str__(self) -> str:
        """String representation o the output."""
        return self.content or ""


TChatInput = TypeVar("TChatInput", bound=ChatLLMInput, covariant=True)
TChatOutput = TypeVar("TChatOutput", bound=ChatLLMOutput, covariant=True)
ChatLLM: TypeAlias = LLM[TChatInput, TChatOutput, THistoryEntry, TModelParameters]
"""Generic Completion LLM type alias."""

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/types/generics.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""LLM generic variables."""

from typing import Any, TypeAlias, TypeVar

from pydantic import BaseModel

TInput = TypeVar("TInput", contravariant=True)
"""Raw input type used for the prompt."""

TOutput = TypeVar("TOutput")
"""Type of the raw output of LLM invocation."""

TJsonModel = TypeVar("TJsonModel", bound=BaseModel)
"""Model type used to parse the raw json output, if available."""

THistoryEntry = TypeVar("THistoryEntry")
"""Type of a history entry (chat mode)."""

TModelParameters = TypeVar("TModelParameters", contravariant=True)
"""Type of the parameters that can be forwarded to the model."""

PromptVariables: TypeAlias = dict[str, Any]
"""Indicates variables that can be to populate the prompt."""

JSON: TypeAlias = dict[Any, Any] | list[Any]
"""JSON represented in a python data structures."""

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/config/config.py ==================
# Copyright (c) 2024 Microsoft Corporation.


"""LLM Configuration Protocol definition."""

from pydantic import BaseModel, Field

from .json_strategy import JsonStrategy


class Config(BaseModel, frozen=True, extra="allow"):
    """Configuration protocol definition."""

    max_retries: int = Field(
        default=10,
        description="The maximum number of retries.",
    )

    max_json_retries: int = Field(
        default=3, description="The maximum number of retries for JSON generation."
    )

    max_retry_wait: float = Field(
        default=10, description="The maximum retry wait time."
    )

    max_concurrency: int | None = Field(
        default=None,
        description="The maximum concurrency. This is the number of concurrent requests that can be made at once.",
    )

    tokens_per_minute: int | None = Field(
        default=None, description="The max number of tokens per minute."
    )

    requests_per_minute: int | None = Field(
        default=None,
        description="The max number of requests per minute.",
    )

    requests_burst_mode: bool = Field(
        default=True, description="Use burst mode when submitting requests."
    )

    json_strategy: JsonStrategy = Field(
        default=JsonStrategy.VALID,
        description="The strategy to use for JSON parsing.",
    )

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/config/json_strategy.py ==================
# Copyright (c) 2024 Microsoft Corporation.


"""LLM Configuration Protocol definition."""

from enum import Enum


class JsonStrategy(str, Enum):
    """The strategy to use for JSON parsing."""

    LOOSE = "loose"
    """Manual JSON parsing. Your prompt should request JSON, and the LLM will attempt to parse it, re-invoking the LLM if necessary to clean up the output."""

    VALID = "valid"
    """The LLM is contractually obligated to return valid JSON, although it may not conform to a schema."""

    STRUCTURED = "structured"
    """The LLM is contractually obligated to return valid JSON that conforms to a schema."""

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/config/__init__.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""Configuration base package."""

from .config import Config
from .json_strategy import JsonStrategy

__all__ = ["Config", "JsonStrategy"]

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/__init__.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""The fnllm package, containing utilities to interact with LLMs."""

from .caching import Cache
from .config import Config, JsonStrategy
from .events import LLMEvents, LLMEventsLogger, LLMUsageTracker
from .limiting import (
    CompositeLimiter,
    ConcurrencyLimiter,
    Limiter,
    NoopLimiter,
    RPMLimiter,
    TPMLimiter,
)
from .tools import LLMTool
from .types import (
    LLM,
    ChatLLM,
    ChatLLMInput,
    ChatLLMOutput,
    EmbeddingsLLM,
    EmbeddingsLLMInput,
    EmbeddingsLLMOutput,
    LLMInput,
    LLMMetrics,
    LLMOutput,
    LLMRetryMetrics,
    LLMUsageMetrics,
)
from .utils.sliding_window import SlidingWindow, SlidingWindowEntry

__all__ = [
    "LLM",
    "Cache",
    "ChatLLM",
    "ChatLLMInput",
    "ChatLLMOutput",
    "ChatLLMOutput",
    "CompositeLimiter",
    "ConcurrencyLimiter",
    "Config",
    "EmbeddingsLLM",
    "EmbeddingsLLMInput",
    "EmbeddingsLLMOutput",
    "JsonStrategy",
    "LLMEvents",
    "LLMEventsLogger",
    "LLMInput",
    "LLMMetrics",
    "LLMOutput",
    "LLMRetryMetrics",
    "LLMTool",
    "LLMUsageMetrics",
    "LLMUsageTracker",
    "Limiter",
    "NoopLimiter",
    "RPMLimiter",
    "SlidingWindow",
    "SlidingWindowEntry",
    "TPMLimiter",
]

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/utils/sliding_window.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""Implementation of a sliding window based on time to calculate a moving average."""

# Copyright (c) 2024 Microsoft Corporation.

from asyncio import Lock
from collections import deque
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone


def get_current_ts() -> datetime:
    """Current timestamp."""
    return datetime.now(timezone.utc)


@dataclass
class SlidingWindowEntry:
    """Represents an entry on the sliding window."""

    ts: datetime
    """Entry timestamp."""

    value: float
    """Entry value."""


# TODO: move this implementation to numpy for better performance
class SlidingWindow:
    """Sliding window implementation. This tracks values for a given time `time_window`, always starting from the current time."""

    def __init__(self, time_window: int):
        """Create a new sliding window with a given `time_window` in seconds.

        For example, if `time_window = 120`, it will keep values for two minutes from when they are inserted.
        """
        self._mutex = Lock()
        self._queue = deque[SlidingWindowEntry]()
        self._time_window = timedelta(seconds=time_window)
        self._time_windows_secs = time_window
        self._total = 0.0
        self._first_insertion_ts: datetime | None = None
        self._last_insertion_ts: datetime | None = None

    def _track_total(self, value: float) -> datetime:
        now = get_current_ts()

        if not self._first_insertion_ts:
            self._first_insertion_ts = now

        self._last_insertion_ts = now
        self._total += value

        return now

    def _remove_outside_window(self, now: datetime):
        while len(self._queue) > 0 and self._queue[0].ts + self._time_window < now:
            # timestamp at the front is outside the window, remove
            self._queue.popleft()

    async def insert(self, value: float) -> None:
        """Insert a new value into window, the timestamp will be the insertion time."""
        async with self._mutex:
            now = self._track_total(value)
            self._queue.append(SlidingWindowEntry(now, value))
            self._remove_outside_window(now)

    async def sum(self) -> float:
        """Get the sum of all values still in the window."""
        async with self._mutex:
            self._remove_outside_window(get_current_ts())
            return sum(entry.value for entry in self._queue)

    async def avg(self) -> float:
        """Average per time window of all the values that have ever been inserted in the window (including the ones already outside it).

        `result = [sum(all values) / (last_insertion_ts - first_insertion_ts)] * time_window`
        """
        async with self._mutex:
            if (
                not self._first_insertion_ts
                or not self._last_insertion_ts
                or self._last_insertion_ts == self._first_insertion_ts
            ):
                return self._total

            diff = (self._last_insertion_ts - self._first_insertion_ts).total_seconds()

            # keep accumulating until enough time has passed
            if diff < self._time_windows_secs:
                return self._total

            return self._total * self._time_windows_secs / diff

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/utils/__init__.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""Utility package."""

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/limiting/concurrency.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""Concurrency limiter module."""

from asyncio import Semaphore

from fnllm.limiting.base import Limiter, Manifest


class ConcurrencyLimiter(Limiter):
    """Concurrency limiter class definition."""

    def __init__(self, semaphore: Semaphore):
        """Create a new ConcurrencyLimiter."""
        self._semaphore = semaphore

    async def acquire(self, manifest: Manifest) -> None:
        """Acquire a concurrency slot."""
        if manifest.request_tokens > 0:
            await self._semaphore.acquire()

    async def release(self, manifest: Manifest) -> None:
        """Release the concurrency slot."""
        if manifest.request_tokens > 0:
            self._semaphore.release()

    @classmethod
    def from_max_concurrency(cls, max_concurrency: int) -> "ConcurrencyLimiter":
        """Create a new ConcurrencyLimiter."""
        return cls(Semaphore(max_concurrency))

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/limiting/__init__.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""Limiting base package."""

from .base import Limiter, Manifest
from .composite import CompositeLimiter
from .concurrency import ConcurrencyLimiter
from .noop_llm import NoopLimiter
from .rpm import RPMLimiter
from .tpm import TPMLimiter

__all__ = [
    "CompositeLimiter",
    "ConcurrencyLimiter",
    "Limiter",
    "Manifest",
    "NoopLimiter",
    "RPMLimiter",
    "TPMLimiter",
]

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/limiting/tpm.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""TPM RPM limiter module."""

from aiolimiter import AsyncLimiter

from fnllm.limiting.base import Limiter, Manifest


class TPMLimiter(Limiter):
    """TPM limiter class definition."""

    def __init__(self, limiter: AsyncLimiter):
        """Create a new RpmLimiter."""
        self._limiter = limiter

    async def acquire(self, manifest: Manifest) -> None:
        """Acquire limiter permission."""
        total_tokens = manifest.request_tokens + manifest.post_request_tokens

        if total_tokens > 0:
            await self._limiter.acquire(total_tokens)

    async def release(self, manifest: Manifest) -> None:
        """Do nothing."""

    @classmethod
    def from_tpm(cls, tokens_per_minute: int) -> "TPMLimiter":
        """Create a new RpmLimiter."""
        return cls(AsyncLimiter(tokens_per_minute))

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/limiting/composite.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""Composite limiter module."""

from collections.abc import Sequence

from .base import Limiter, Manifest


class CompositeLimiter(Limiter):
    """A composite limiter that combines multiple limiters."""

    def __init__(self, limiters: Sequence[Limiter]):
        """A composite limiter that combines multiple limiters."""
        self._limiters = limiters
        self._acquire_order = limiters
        self._release_order = limiters[::-1]

    async def acquire(self, manifest: Manifest) -> None:
        """Acquire the specified amount of tokens from all limiters."""
        # this needs to be sequential, the order of the limiters must be respected
        # to avoid deadlocks
        for limiter in self._acquire_order:
            await limiter.acquire(manifest)

    async def release(self, manifest: Manifest) -> None:
        """Release all tokens from all limiters."""
        # release in the opposite order we acquired
        # the last limiter acquired should be the first one released
        for limiter in self._release_order:
            await limiter.release(manifest)

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/limiting/rpm.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""TPM RPM limiter module."""

from aiolimiter import AsyncLimiter

from fnllm.limiting.base import Limiter, Manifest


class RPMLimiter(Limiter):
    """RPM limiter class definition."""

    def __init__(self, limiter: AsyncLimiter):
        """Create a new RPMLimiter."""
        self._limiter = limiter

    async def acquire(self, manifest: Manifest) -> None:
        """Acquire a new request."""
        if manifest.request_tokens > 0:
            await self._limiter.acquire()

    async def release(self, manifest: Manifest) -> None:
        """Do nothing."""

    @classmethod
    def from_rpm(
        cls, requests_per_minute: int, burst_mode: bool = True
    ) -> "RPMLimiter":
        """Create a new RPMLimiter."""
        if burst_mode:
            return cls(AsyncLimiter(requests_per_minute, time_period=60))

        return cls(AsyncLimiter(1, time_period=60 / requests_per_minute))

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/limiting/noop_llm.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""Noop limiter module."""

from fnllm.limiting.base import Limiter, Manifest


class NoopLimiter(Limiter):
    """Noop limiter class definition."""

    async def acquire(self, manifest: Manifest) -> None:
        """Do nothing."""

    async def release(self, manifest: Manifest) -> None:
        """Do nothing."""

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/limiting/base.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""Base limiter interface."""

from abc import ABC, abstractmethod
from dataclasses import dataclass
from types import TracebackType


@dataclass
class Manifest:
    """Parameters for limiting."""

    request_tokens: int = 0
    """The number of tokens to acquire or release."""

    post_request_tokens: int = 0
    """The number of tokens to acquire or release after the request is complete."""


class LimitContext:
    """A context manager for limiting."""

    def __init__(self, limiter: "Limiter", manifest: Manifest):
        """Create a new LimitContext."""
        self._limiter = limiter
        self._manifest = manifest

    async def __aenter__(self) -> "LimitContext":
        """Enter the context."""
        await self._limiter.acquire(self._manifest)
        return self

    async def __aexit__(
        self,
        exc_type: type[BaseException] | None,
        exc_value: BaseException | None,
        traceback: TracebackType | None,
    ) -> None:
        """Exit the context."""
        await self._limiter.release(self._manifest)


class Limiter(ABC):
    """Limiter interface."""

    @abstractmethod
    async def acquire(self, manifest: Manifest) -> None:
        """Acquire a pass through the limiter."""

    @abstractmethod
    async def release(self, manifest: Manifest) -> None:
        """Release a pass through the limiter."""

    def use(self, manifest: Manifest) -> LimitContext:
        """Limit for a given amount (default = 1)."""
        return LimitContext(self, manifest)

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/events/__init__.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""Package with utilities for LLM event handling."""

from .base import LLMEvents
from .composite import LLMCompositeEvents
from .logger import LLMEventsLogger
from .usage_tracker import LLMUsageTracker

__all__ = ["LLMCompositeEvents", "LLMEvents", "LLMEventsLogger", "LLMUsageTracker"]

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/events/usage_tracker.py ==================
# Copyright (c) 2024 Microsoft Corporation.
# Licensed under the MIT License

"""Class for LLM event usage tracking."""

from fnllm.events.base import LLMEvents
from fnllm.limiting.base import Manifest
from fnllm.types.metrics import LLMUsageMetrics
from fnllm.utils.sliding_window import SlidingWindow


class LLMUsageTracker(LLMEvents):
    """Implementation of the LLM events to track usage information."""

    def __init__(
        self,
        rpm_sliding_window: SlidingWindow,
        tpm_sliding_window: SlidingWindow,
    ) -> None:
        """Create a new LLMUsageTracker."""
        self._rpm_sliding_window = rpm_sliding_window
        self._tpm_sliding_window = tpm_sliding_window
        self._current_concurrency = 0
        self._max_concurrency = 0
        self._total_usage = LLMUsageMetrics()
        self._total_requests = 0

    @property
    def total_usage(self) -> LLMUsageMetrics:
        """Total usage so far."""
        return self._total_usage.model_copy()

    @property
    def total_requests(self) -> int:
        """Total number of requests made."""
        return self._total_requests

    @property
    def current_concurrency(self) -> int:
        """Current effective concurrency."""
        return self._current_concurrency

    @property
    def max_concurrency(self) -> int:
        """Maximum registered concurrency."""
        return self._max_concurrency

    async def current_rpm(self) -> float:
        """Returns the current RPM for `[now - time_window, now]`."""
        return await self._rpm_sliding_window.sum()

    async def avg_rpm(self) -> float:
        """Return the total average RPM since the beginning."""
        return await self._rpm_sliding_window.avg()

    async def current_tpm(self) -> float:
        """Returns the current TPM from `[now - time_window, now]`."""
        return await self._tpm_sliding_window.sum()

    async def avg_tpm(self) -> float:
        """Return the total average TPM since the beginning."""
        return await self._tpm_sliding_window.avg()

    async def on_usage(self, usage: LLMUsageMetrics) -> None:
        """Called when there is any LLM usage."""
        self._total_requests += 1
        self._total_usage.input_tokens += usage.input_tokens
        self._total_usage.output_tokens += usage.output_tokens

    async def on_limit_acquired(self, manifest: Manifest) -> None:
        """Called when limit is acquired for a request (does not include post limiting)."""
        self._current_concurrency += 1
        self._max_concurrency = max(self._max_concurrency, self._current_concurrency)

        await self._rpm_sliding_window.insert(1)
        await self._tpm_sliding_window.insert(manifest.request_tokens)

    async def on_limit_released(self, manifest: Manifest) -> None:
        """Called when limit is released for a request (does not include post limiting)."""
        self._current_concurrency = max(0, self._current_concurrency - 1)

    async def on_post_limit(self, manifest: Manifest) -> None:
        """Called when post request limiting is triggered (called by the rate limiting LLM)."""
        await self._tpm_sliding_window.insert(manifest.post_request_tokens)

    @classmethod
    def create(cls) -> "LLMUsageTracker":
        """Create a new LLMUsageTracker with proper sliding windows."""
        return cls(SlidingWindow(60), SlidingWindow(60))

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/events/logger.py ==================
# Copyright (c) 2024 Microsoft Corporation.
# Licensed under the MIT License

"""Class for LLM event logging."""

from logging import Logger
from typing import Any

from fnllm.events.base import LLMEvents
from fnllm.limiting.base import Manifest
from fnllm.types.metrics import LLMMetrics, LLMUsageMetrics


class LLMEventsLogger(LLMEvents):
    """Implementation of the LLM events that just logs the events."""

    def __init__(self, logger: Logger) -> None:
        """Create a new LLMEventsLogger."""
        self._logger = logger

    async def on_error(
        self,
        error: BaseException | None,
        traceback: str | None = None,
        arguments: dict[str, Any] | None = None,
    ) -> None:
        """An unhandled error that happens during the LLM call (called by the LLM base)."""
        self._logger.error(
            "unexpected error occurred for arguments '%s':\n\n%s\n\n%s",
            arguments,
            error,
            traceback,
        )

    async def on_usage(self, usage: LLMUsageMetrics) -> None:
        """Called when there is any LLM usage."""
        self._logger.info(
            "LLM usage with %d total tokens (input=%d, output=%d)",
            usage.total_tokens,
            usage.input_tokens,
            usage.output_tokens,
        )

    async def on_limit_acquired(self, manifest: Manifest) -> None:
        """Called when limit is acquired for a request (does not include post limiting)."""
        self._logger.info(
            "limit acquired for request, request_tokens=%d, post_request_tokens=%d",
            manifest.request_tokens,
            manifest.post_request_tokens,
        )

    async def on_limit_released(self, manifest: Manifest) -> None:
        """Called when limit is released for a request (does not include post limiting)."""
        self._logger.info(
            "limit released for request, request_tokens=%d, post_request_tokens=%d",
            manifest.request_tokens,
            manifest.post_request_tokens,
        )

    async def on_post_limit(self, manifest: Manifest) -> None:
        """Called when post request limiting is triggered (called by the rate limiting LLM)."""
        self._logger.info(
            "post request limiting triggered, acquired extra %d tokens",
            manifest.post_request_tokens,
        )

    async def on_success(
        self,
        metrics: LLMMetrics,
    ) -> None:
        """Called when a request goes through (called by the retrying LLM)."""
        self._logger.info(
            "request succeed with %d retries in %.2fs and used %d tokens",
            metrics.retry.num_retries,
            metrics.retry.total_time,
            metrics.usage.total_tokens,
        )

    async def on_cache_hit(self, cache_key: str, name: str | None) -> None:
        """Called when there is a cache hit."""
        self._logger.info(
            "cache hit for key=%s and name=%s",
            cache_key,
            name,
        )

    async def on_cache_miss(self, cache_key: str, name: str | None) -> None:
        """Called when there is a cache miss."""
        self._logger.info(
            "cache miss for key=%s and name=%s",
            cache_key,
            name,
        )

    async def on_try(self, attempt_number: int) -> None:
        """Called every time a new try to call the LLM happens."""
        self._logger.debug("calling llm, attempt #%d", attempt_number)

    async def on_retryable_error(
        self, error: BaseException, attempt_number: int
    ) -> None:
        """Called when retryable errors happen."""
        self._logger.warning(
            "retryable error happened on attempt #%d: %s", attempt_number, str(error)
        )

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/events/composite.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""Class for LLM composite event handling."""

import asyncio
from collections.abc import Sequence
from typing import Any

from fnllm.events.base import LLMEvents
from fnllm.limiting.base import Manifest
from fnllm.types.metrics import LLMMetrics, LLMUsageMetrics


class LLMCompositeEvents(LLMEvents):
    """Provide support for different events at the same time."""

    def __init__(self, handlers: Sequence[LLMEvents]) -> None:
        """Create a new LLMCompositeEvents."""
        self._handlers = handlers

    async def on_execute_llm(
        self,
    ) -> None:
        """Hook called before the actual LLM call."""
        await asyncio.gather(*[handler.on_execute_llm() for handler in self._handlers])

    async def on_error(
        self,
        error: BaseException | None,
        traceback: str | None = None,
        arguments: dict[str, Any] | None = None,
    ) -> None:
        """An unhandled error that happens during the LLM call (called by the LLM base)."""
        await asyncio.gather(*[
            handler.on_error(error, traceback, arguments) for handler in self._handlers
        ])

    async def on_usage(self, usage: LLMUsageMetrics) -> None:
        """Called when there is any LLM usage."""
        await asyncio.gather(*[handler.on_usage(usage) for handler in self._handlers])

    async def on_limit_acquired(self, manifest: Manifest) -> None:
        """Called when limit is acquired for a request (does not include post limiting)."""
        await asyncio.gather(*[
            handler.on_limit_acquired(manifest) for handler in self._handlers
        ])

    async def on_limit_released(self, manifest: Manifest) -> None:
        """Called when limit is released for a request (does not include post limiting)."""
        await asyncio.gather(*[
            handler.on_limit_released(manifest) for handler in self._handlers
        ])

    async def on_post_limit(self, manifest: Manifest) -> None:
        """Called when post request limiting is triggered (called by the rate limiting LLM)."""
        await asyncio.gather(*[
            handler.on_post_limit(manifest) for handler in self._handlers
        ])

    async def on_success(
        self,
        metrics: LLMMetrics,
    ) -> None:
        """Called when a request goes through (called by the retrying LLM)."""
        await asyncio.gather(*[
            handler.on_success(metrics) for handler in self._handlers
        ])

    async def on_cache_hit(self, cache_key: str, name: str | None) -> None:
        """Called when there is a cache hit."""
        await asyncio.gather(*[
            handler.on_cache_hit(cache_key, name) for handler in self._handlers
        ])

    async def on_cache_miss(self, cache_key: str, name: str | None) -> None:
        """Called when there is a cache miss."""
        await asyncio.gather(*[
            handler.on_cache_miss(cache_key, name) for handler in self._handlers
        ])

    async def on_try(self, attempt_number: int) -> None:
        """Called every time a new try to call the LLM happens."""
        await asyncio.gather(*[
            handler.on_try(attempt_number) for handler in self._handlers
        ])

    async def on_retryable_error(
        self, error: BaseException, attempt_number: int
    ) -> None:
        """Called when retryable errors happen."""
        await asyncio.gather(*[
            handler.on_retryable_error(error, attempt_number)
            for handler in self._handlers
        ])

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/events/base.py ==================
# Copyright (c) 2024 Microsoft Corporation.
# Licensed under the MIT License

"""Base class for LLM event handling."""

from typing import Any

from fnllm.limiting.base import Manifest
from fnllm.types.metrics import LLMMetrics, LLMUsageMetrics


class LLMEvents:
    """Base LLM events handler to be implemented by the user."""

    async def on_execute_llm(
        self,
    ) -> None:
        """Hook called before the actual LLM call."""

    async def on_error(
        self,
        error: BaseException | None,
        traceback: str | None = None,
        arguments: dict[str, Any] | None = None,
    ) -> None:
        """An unhandled error that happens during the LLM call (called by the LLM base)."""

    async def on_usage(self, usage: LLMUsageMetrics) -> None:
        """Called when there is any LLM usage."""

    async def on_limit_acquired(self, manifest: Manifest) -> None:
        """Called when limit is acquired for a request (does not include post limiting)."""

    async def on_limit_released(self, manifest: Manifest) -> None:
        """Called when limit is released for a request (does not include post limiting)."""

    async def on_post_limit(self, manifest: Manifest) -> None:
        """Called when post request limiting is triggered (called by the rate limiting LLM)."""

    async def on_success(
        self,
        metrics: LLMMetrics,
    ) -> None:
        """Called when a request goes through (called by the retrying LLM)."""

    async def on_cache_hit(self, cache_key: str, name: str | None) -> None:
        """Called when there is a cache hit."""

    async def on_cache_miss(self, cache_key: str, name: str | None) -> None:
        """Called when there is a cache miss."""

    async def on_try(self, attempt_number: int) -> None:
        """Called every time a new try to call the LLM happens."""

    async def on_retryable_error(
        self, error: BaseException, attempt_number: int
    ) -> None:
        """Called when retryable errors happen."""

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/caching/__init__.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""Caching base package."""

from .base import Cache
from .file import FileCache

__all__ = ["Cache", "FileCache"]

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/caching/file.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""File cache implementation for the `Cache` protocol.."""

import json
import logging
from pathlib import Path
from typing import Any

from fnllm.caching.base import Cache

_log = logging.getLogger(__name__)


class FileCache(Cache):
    """The FileCache class."""

    def __init__(self, cache_path: Path | str, encoding: str | None = None):
        """Initialize the cache."""
        if isinstance(cache_path, str):
            cache_path = Path(cache_path)

        self._cache_path = cache_path
        self._cache_path.mkdir(exist_ok=True, parents=True)
        self._encoding = encoding or "utf-8"

    @property
    def root_path(self) -> Path:
        """Cache path in the filesystem."""
        return self._cache_path

    async def has(self, key: str) -> bool:
        """Check if the cache has a value."""
        return (self._cache_path / key).exists()

    async def get(self, key: str) -> Any | None:
        """Retrieve a value from the cache."""
        path = self._cache_path / key

        if not path.exists():
            return None

        # throw if result is None
        return json.loads(path.read_text(encoding=self._encoding))["result"]

    async def remove(self, key: str) -> None:
        """Remove a value from the cache."""
        (self._cache_path / key).unlink()

    async def clear(self) -> None:
        """Clear the cache."""
        _clear_dir(self._cache_path)

    async def set(
        self, key: str, value: Any, metadata: dict[str, Any] | None = None
    ) -> None:
        """Write a value into the cache."""
        content = json.dumps(
            {"result": value, "metadata": metadata}, indent=2, ensure_ascii=False
        )
        (self._cache_path / key).write_text(
            content,
            encoding=self._encoding,
        )

    def child(self, key: str) -> "FileCache":
        """Create a child cache."""
        return FileCache(self._cache_path / key)


def _clear_dir(path: Path) -> None:
    """Clear a directory."""
    _log.debug("removing path %s", path)

    for f in path.iterdir():
        if f.is_dir():
            _clear_dir(f)
            f.rmdir()
        else:
            _log.debug("removing file %s", f)
            f.unlink()

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/caching/blob.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""Azure Blob Storage Cache."""

import json
import re
from pathlib import Path
from typing import Any

from azure.core.exceptions import ResourceNotFoundError
from azure.storage.blob import BlobClient, BlobServiceClient, ContainerClient

from .base import Cache


class InvalidBlobContainerNameError(ValueError):
    """Raised when an invalid blob container name is provided."""

    def __init__(self, message: str):
        """Create a new InvalidBlobContainerNameError."""
        super().__init__(message)


class InvalidBlobCacheArgumentsError(ValueError):
    """Raised when invalid blob cache arguments are provided."""

    def __init__(self, message: str):
        """Create a new InvalidBlobContainerNameError."""
        super().__init__(message)


class BlobCache(Cache):
    """The Blob-Storage implementation."""

    _connection_string: str | None
    _container_name: str
    _path_prefix: str
    _encoding: str
    _storage_account_blob_url: str | None

    def __init__(
        self,
        *,
        connection_string: str | None = None,
        container_name: str,
        encoding: str | None = None,
        path_prefix: str | None = None,
        storage_account_blob_url: str | None = None,
    ):
        """Create a new BlobStorage instance."""
        if connection_string:
            self._blob_service_client = BlobServiceClient.from_connection_string(
                connection_string
            )
        else:
            from azure.identity import DefaultAzureCredential

            if storage_account_blob_url is None:
                msg = "Either connection_string or storage_account_blob_url must be provided."
                raise InvalidBlobCacheArgumentsError(msg)

            self._blob_service_client = BlobServiceClient(
                account_url=storage_account_blob_url,
                credential=DefaultAzureCredential(),
            )

        validate_blob_container_name(container_name)
        self._encoding = encoding or "utf-8"
        self._container_name = container_name
        self._connection_string = connection_string
        self._path_prefix = path_prefix or ""
        self._storage_account_blob_url = storage_account_blob_url
        self._storage_account_name = (
            storage_account_blob_url.split("//")[1].split(".")[0]
            if storage_account_blob_url
            else None
        )
        self.create_container()

    @property
    def container_name(self) -> str:
        """Get the container name."""
        return self._container_name

    @property
    def blob_service_client(self) -> BlobServiceClient:
        """Get the blob service client."""
        return self._blob_service_client

    @property
    def container_client(self) -> ContainerClient:
        """Get the container client."""
        return self.blob_service_client.get_container_client(self.container_name)

    def blob_client(self, name: str) -> BlobClient:
        """Get a blob client."""
        return self.container_client.get_blob_client(name)

    def container_exists(self) -> bool:
        """Check if the container exists."""
        container_name = self.container_name
        container_names = [
            container.name for container in self.blob_service_client.list_containers()
        ]
        return container_name in container_names

    def create_container(self) -> None:
        """Create the container if it does not exist."""
        if not self.container_exists():
            self.blob_service_client.create_container(self.container_name)

    def delete_container(self) -> None:
        """Delete the container."""
        if self.container_exists():
            self.blob_service_client.delete_container(self.container_name)

    async def has(self, key: str) -> bool:
        """Check if a key exists in the cache."""
        key = self._keyname(key)
        blob_client = self.blob_client(key)
        return blob_client.exists()

    async def get(self, key: str) -> Any | None:
        """Get a value from the cache."""
        try:
            key = self._keyname(key)
            blob_client = self.blob_client(key)
            blob_data = blob_client.download_blob().readall()
        except ResourceNotFoundError:
            return None
        else:
            blob_data = blob_data.decode(self._encoding)
            data = json.loads(blob_data)
            return data["result"]

    async def set(
        self, key: str, value: Any, metadata: dict[str, Any] | None = None
    ) -> None:
        """Set a value in the cache."""
        key = self._keyname(key)
        content = json.dumps(
            {"result": value, "metadata": metadata}, indent=2, ensure_ascii=False
        )
        blob_client = self.blob_client(key)
        blob_client.upload_blob(content.encode(self._encoding), overwrite=True)

    async def remove(self, key: str) -> None:
        """Delete a key from the cache."""
        key = self._keyname(key)
        blob_client = self.blob_client(key)
        blob_client.delete_blob()

    async def clear(self) -> None:
        """Clear the cache."""
        for blob in [*self.container_client.list_blob_names()]:
            self.blob_client(blob).delete_blob()

    def child(self, key: str) -> "BlobCache":
        """Create a child storage instance."""
        path = str(Path(self._path_prefix) / key)
        return BlobCache(
            connection_string=self._connection_string,
            container_name=self.container_name,
            encoding=self._encoding,
            path_prefix=path,
            storage_account_blob_url=self._storage_account_blob_url,
        )

    def _keyname(self, key: str) -> str:
        """Get the key name."""
        return str(Path(self._path_prefix) / key)


def validate_blob_container_name(container_name: str) -> bool:
    """Check if the provided blob container name is valid based on Azure rules.

        - A blob container name must be between 3 and 63 characters in length.
        - Start with a letter or number
        - All letters used in blob container names must be lowercase.
        - Contain only letters, numbers, or the hyphen.
        - Consecutive hyphens are not permitted.
        - Cannot end with a hyphen.


    container_name (str)
        The blob container name to be validated.

    Returns
    -------
        bool: True if valid, raises otherwise.
    """
    # Check the length of the name
    if len(container_name) < 3 or len(container_name) > 63:
        msg = f"Container name must be between 3 and 63 characters in length. Name provided was {len(container_name)} characters long."
        raise InvalidBlobContainerNameError(msg)

    # Check if the name starts with a letter or number
    if not container_name[0].isalnum():
        msg = f"Container name must start with a letter or number. Starting character was {container_name[0]}."
        raise InvalidBlobContainerNameError(msg)

    # Check for valid characters (letters, numbers, hyphen) and lowercase letters
    if not re.match("^[a-z0-9-]+$", container_name):
        msg = f"Container name must only contain:\n- lowercase letters\n- numbers\n- or hyphens\nName provided was {container_name}."
        raise InvalidBlobContainerNameError(msg)

    # Check for consecutive hyphens
    if "--" in container_name:
        msg = f"Container name cannot contain consecutive hyphens. Name provided was {container_name}."
        raise InvalidBlobContainerNameError(msg)

    # Check for hyphens at the end of the name
    if container_name[-1] == "-":
        msg = f"Container name cannot end with a hyphen. Name provided was {container_name}."
        raise InvalidBlobContainerNameError(msg)

    return True

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/caching/base.py ==================
# Copyright (c) 2024 Microsoft Corporation.


"""Cache protocol definition."""

import hashlib
import json
from abc import ABC, abstractmethod
from typing import Any


class Cache(ABC):
    """Cache base class."""

    __cache_strategy_version__: int = 2
    """If there's a breaking change in what we cache, we should increment this version number to invalidate existing caches."""

    @abstractmethod
    async def has(self, key: str) -> bool:
        """Check if the cache has a value."""

    @abstractmethod
    async def get(self, key: str) -> Any | None:
        """Retrieve a value from the cache."""

    @abstractmethod
    async def remove(self, key: str) -> None:
        """Remove a value from the cache."""

    @abstractmethod
    async def clear(self) -> None:
        """Clear the cache."""

    @abstractmethod
    async def set(
        self, key: str, value: Any, metadata: dict[str, Any] | None = None
    ) -> None:
        """Write a value into the cache."""

    @abstractmethod
    def child(self, key: str) -> "Cache":
        """Create a child cache."""

    def create_key(self, data: Any, *, prefix: str | None = None) -> str:
        """Create a custom key by hashing the data. Returns `{data_hash}_v{strategy_version}` or `{prefix}_{data_hash}_v{strategy_version}`."""
        data_hash = _hash(json.dumps(data, sort_keys=True))

        if prefix is not None:
            return f"{prefix}_{data_hash}_v{self.__cache_strategy_version__}"

        return f"{data_hash}_v{self.__cache_strategy_version__}"


def _hash(_input: str) -> str:
    """Use a deterministic hashing approach."""
    return hashlib.sha256(_input.encode()).hexdigest()

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/base/__init__.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""LLM base package."""

from .base import BaseLLM

__all__ = ["BaseLLM"]

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/base/base.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""Base LLM module."""

import traceback
from abc import ABC, abstractmethod
from collections.abc import Sequence
from typing import Generic

from typing_extensions import Unpack

from fnllm.caching.base import Cache
from fnllm.events.base import LLMEvents
from fnllm.services.decorator import LLMDecorator
from fnllm.services.history_extractor import HistoryExtractor
from fnllm.services.json import JsonHandler
from fnllm.services.rate_limiter import RateLimiter
from fnllm.services.retryer import Retryer
from fnllm.services.usage_extractor import UsageExtractor
from fnllm.services.variable_injector import VariableInjector
from fnllm.types.generics import (
    THistoryEntry,
    TInput,
    TJsonModel,
    TModelParameters,
    TOutput,
)
from fnllm.types.io import LLMInput, LLMOutput
from fnllm.types.metrics import LLMUsageMetrics
from fnllm.types.protocol import LLM


class BaseLLM(
    ABC,
    LLM[TInput, TOutput, THistoryEntry, TModelParameters],
    Generic[TInput, TOutput, THistoryEntry, TModelParameters],
):
    """Base LLM interface definition."""

    def __init__(
        self,
        *,
        events: LLMEvents | None = None,
        cache: Cache | None = None,
        usage_extractor: UsageExtractor[TOutput] | None = None,
        history_extractor: HistoryExtractor[TOutput, THistoryEntry] | None = None,
        variable_injector: VariableInjector | None = None,
        rate_limiter: RateLimiter[TInput, TOutput, THistoryEntry, TModelParameters]
        | None = None,
        retryer: Retryer[TInput, TOutput, THistoryEntry, TModelParameters]
        | None = None,
        json_handler: JsonHandler[TOutput, THistoryEntry] | None = None,
    ) -> None:
        """Base constructor for the BaseLLM."""
        self._events = events or LLMEvents()
        self._cache = cache
        self._usage_extractor = usage_extractor
        self._history_extractor = history_extractor
        self._variable_injector = variable_injector
        self._rate_limiter = rate_limiter
        self._retryer = retryer
        self._json_handler = json_handler

        decorated = self._decorator_target
        for decorator in self.decorators:
            decorated = decorator.decorate(decorated)
        self._decorated_target = decorated

    def child(
        self, name: str
    ) -> "BaseLLM[TInput, TOutput, THistoryEntry, TModelParameters]":
        """Create a child LLM."""
        if self._cache is None:
            return self
        return self.__class__(
            events=self._events,
            cache=self._cache.child(name),
            usage_extractor=self._usage_extractor,
            history_extractor=self._history_extractor,
            variable_injector=self._variable_injector,
            rate_limiter=self._rate_limiter,
            retryer=self._retryer,
            json_handler=self._json_handler,
        )

    @property
    def events(self) -> LLMEvents:
        """Registered LLM events handler."""
        return self._events

    @property
    def decorators(self) -> list[LLMDecorator[TOutput, THistoryEntry]]:
        """Get the list of LLM decorators."""
        decorators: list[LLMDecorator] = []
        if self._json_handler and self._json_handler.requester:
            decorators.append(self._json_handler.requester)
        if self._rate_limiter:
            decorators.append(self._rate_limiter)
        if self._retryer:
            decorators.append(self._retryer)
        if self._json_handler and self._json_handler.receiver:
            decorators.append(self._json_handler.receiver)
        return decorators

    async def __call__(
        self,
        prompt: TInput,
        **kwargs: Unpack[LLMInput[TJsonModel, THistoryEntry, TModelParameters]],
    ) -> LLMOutput[TOutput, TJsonModel, THistoryEntry]:
        """Invoke the LLM."""
        try:
            return await self._invoke(prompt, **kwargs)
        except BaseException as e:
            stack_trace = traceback.format_exc()
            if self._events:
                await self._events.on_error(
                    e, stack_trace, {"prompt": prompt, "kwargs": kwargs}
                )
            raise

    async def _invoke(
        self,
        prompt: TInput,
        **kwargs: Unpack[LLMInput[TJsonModel, THistoryEntry, TModelParameters]],
    ) -> LLMOutput[TOutput, TJsonModel, THistoryEntry]:
        """Run the LLM invocation, returning an LLMOutput."""
        prompt, kwargs = self._rewrite_input(prompt, kwargs)
        return await self._decorated_target(prompt, **kwargs)

    def _rewrite_input(
        self,
        prompt: TInput,
        kwargs: LLMInput[TJsonModel, THistoryEntry, TModelParameters],
    ) -> tuple[TInput, LLMInput[TJsonModel, THistoryEntry, TModelParameters]]:
        """Rewrite the input prompt and arguments.."""
        if self._variable_injector:
            prompt = self._variable_injector.inject_variables(
                prompt, kwargs.get("variables")
            )
        return prompt, kwargs

    async def _decorator_target(
        self,
        prompt: TInput,
        **kwargs: Unpack[LLMInput[TJsonModel, THistoryEntry, TModelParameters]],
    ) -> LLMOutput[TOutput, TJsonModel, THistoryEntry]:
        """Target for the decorator chain.

        Leave signature alone as prompt, **kwargs.
        """
        await self._events.on_execute_llm()
        output = await self._execute_llm(prompt, **kwargs)
        result: LLMOutput[TOutput, TJsonModel, THistoryEntry] = LLMOutput(output=output)

        await self._inject_usage(result)
        self._inject_history(result, kwargs.get("history"))

        return result

    async def _inject_usage(
        self, result: LLMOutput[TOutput, TJsonModel, THistoryEntry]
    ):
        usage = LLMUsageMetrics()
        if self._usage_extractor:
            usage = self._usage_extractor.extract_usage(result.output)
            await self._events.on_usage(usage)
        result.metrics.usage = usage

    def _inject_history(
        self,
        result: LLMOutput[TOutput, TJsonModel, THistoryEntry],
        history: Sequence[THistoryEntry] | None,
    ) -> None:
        if self._history_extractor:
            result.history = self._history_extractor.extract_history(
                history, result.output
            )

    @abstractmethod
    async def _execute_llm(
        self,
        prompt: TInput,
        **kwargs: Unpack[LLMInput[TJsonModel, THistoryEntry, TModelParameters]],
    ) -> TOutput: ...

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/services/decorator.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""Base protocol for decorator services."""

from collections.abc import Awaitable, Callable
from typing import Generic, Protocol, runtime_checkable

from fnllm.types.generics import THistoryEntry, TJsonModel, TOutput
from fnllm.types.io import LLMOutput


@runtime_checkable
class LLMDecorator(Protocol, Generic[TOutput, THistoryEntry]):
    """A decorator for LLM calls."""

    def decorate(
        self,
        delegate: Callable[
            ..., Awaitable[LLMOutput[TOutput, TJsonModel, THistoryEntry]]
        ],
    ) -> Callable[..., Awaitable[LLMOutput[TOutput, TJsonModel, THistoryEntry]]]:
        """Decorate the delegate with the LLM functionality."""
        ...

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/services/usage_extractor.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""LLM cache-interactor module."""

from abc import ABC, abstractmethod
from typing import Generic

from fnllm.types import LLMUsageMetrics
from fnllm.types.generics import TOutput


class UsageExtractor(ABC, Generic[TOutput]):
    """Usage extractor base class."""

    @abstractmethod
    def extract_usage(
        self,
        output: TOutput,
    ) -> LLMUsageMetrics:
        """Extract LLM usage from the output."""

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/services/rate_limiter.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""Rate limiting LLM implementation."""

from abc import abstractmethod
from collections.abc import Awaitable, Callable
from typing import Any, Generic

from typing_extensions import Unpack

from fnllm.events.base import LLMEvents
from fnllm.limiting import Limiter, Manifest
from fnllm.types.generics import TInput, TJsonModel, TModelParameters
from fnllm.types.io import LLMInput, LLMOutput

from .decorator import LLMDecorator, THistoryEntry, TOutput


class RateLimiter(
    LLMDecorator[TOutput, THistoryEntry],
    Generic[TInput, TOutput, THistoryEntry, TModelParameters],
):
    """A base class to rate limit the LLM."""

    def __init__(
        self,
        limiter: Limiter,
        *,
        events: LLMEvents | None = None,
    ):
        """Create a new BaseRateLimitLLM."""
        self._limiter = limiter
        self._events = events or LLMEvents()

    @abstractmethod
    def _estimate_request_tokens(
        self,
        prompt: TInput,
        kwargs: LLMInput[TJsonModel, THistoryEntry, TModelParameters],
    ) -> int:
        """Estimate how many tokens are on the request input."""

    async def _handle_post_request_limiting(
        self,
        result: LLMOutput[TOutput, TJsonModel, THistoryEntry],
    ) -> None:
        diff = result.metrics.tokens_diff

        if diff > 0:
            manifest = Manifest(post_request_tokens=diff)
            # consume the token difference
            async with self._limiter.use(manifest):
                await self._events.on_post_limit(manifest)

    def decorate(
        self,
        delegate: Callable[
            ..., Awaitable[LLMOutput[TOutput, TJsonModel, THistoryEntry]]
        ],
    ) -> Callable[..., Awaitable[LLMOutput[TOutput, TJsonModel, THistoryEntry]]]:
        """Execute the LLM with the configured rate limits."""

        async def invoke(prompt: TInput, **args: Unpack[LLMInput[Any, Any, Any]]):
            estimated_input_tokens = self._estimate_request_tokens(prompt, args)

            manifest = Manifest(request_tokens=estimated_input_tokens)
            try:
                async with self._limiter.use(manifest):
                    await self._events.on_limit_acquired(manifest)
                    result = await delegate(prompt, **args)
            finally:
                await self._events.on_limit_released(manifest)

            result.metrics.estimated_input_tokens = estimated_input_tokens
            await self._handle_post_request_limiting(result)

            return result

        return invoke

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/services/__init__.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""LLM Services."""

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/services/cache_interactor.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""LLM cache-interactor module."""

from collections.abc import Awaitable, Callable
from typing import Any

from fnllm.caching.base import Cache
from fnllm.events.base import LLMEvents
from fnllm.types.generics import TJsonModel


class CacheInteractor:
    """A cache interactor class."""

    def __init__(
        self, events: LLMEvents | None = None, cache: Cache | None = None
    ) -> None:
        """Base constructor for the BaseLLM."""
        self._events = events or LLMEvents()
        self._cache = cache

    def child(self, name: str) -> "CacheInteractor":
        """Create a child cache interactor."""
        if self._cache is None:
            return self
        return CacheInteractor(events=self._events, cache=self._cache.child(name))

    async def get_or_insert(
        self,
        func: Callable[[], Awaitable[TJsonModel]],
        *,
        prefix: str,
        key_data: dict[str, Any],
        name: str | None,
        json_model: type[TJsonModel],
        bypass_cache: bool = False,
    ) -> TJsonModel:
        """Get or insert an item into the cache."""
        if not self._cache or bypass_cache:
            return await func()

        key = self._cache.create_key(key_data, prefix=prefix)
        cached_value = await self._cache.get(key)

        if cached_value:
            entry = json_model.model_validate(cached_value)
            await self._events.on_cache_hit(key, name)
        else:
            entry = await func()
            await self._cache.set(key, entry.model_dump(), {"input": key_data})
            await self._events.on_cache_miss(key, name)

        return entry

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/services/retryer.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""Rate limiting LLM implementation."""

import asyncio
from abc import abstractmethod
from collections.abc import Awaitable, Callable, Sequence
from typing import Any, Generic

from tenacity import (
    AsyncRetrying,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential_jitter,
)
from typing_extensions import Unpack

from fnllm.events.base import LLMEvents
from fnllm.services.errors import RetriesExhaustedError
from fnllm.types.generics import (
    THistoryEntry,
    TInput,
    TJsonModel,
    TModelParameters,
    TOutput,
)
from fnllm.types.io import LLMInput, LLMOutput
from fnllm.types.metrics import LLMRetryMetrics

from .decorator import LLMDecorator


class Retryer(
    LLMDecorator[TOutput, THistoryEntry],
    Generic[TInput, TOutput, THistoryEntry, TModelParameters],
):
    """A base class to add retries to an llm."""

    def __init__(
        self,
        *,
        retryable_errors: Sequence[type[Exception]],
        tag: str = "RetryingLLM",
        max_retries: int = 10,
        max_retry_wait: float = 10,
        events: LLMEvents | None = None,
    ):
        """Create a new RetryingLLM."""
        self._retryable_errors = retryable_errors
        self._tag = tag
        self._max_retries = max_retries
        self._max_retry_wait = max_retry_wait
        self._events = events or LLMEvents()

    @abstractmethod
    async def _on_retryable_error(self, error: BaseException) -> None:
        """Called as soon as retryable error happen."""

    def decorate(
        self,
        delegate: Callable[
            ..., Awaitable[LLMOutput[TOutput, TJsonModel, THistoryEntry]]
        ],
    ) -> Callable[..., Awaitable[LLMOutput[TOutput, TJsonModel, THistoryEntry]]]:
        """Execute the LLM with the configured rate limits."""

        async def invoke(prompt: TInput, **kwargs: Unpack[LLMInput[Any, Any, Any]]):
            name = kwargs.get("name", self._tag)
            attempt_number = 0
            call_times: list[float] = []

            async def attempt() -> LLMOutput[TOutput, TJsonModel, THistoryEntry]:
                nonlocal call_times
                call_start = asyncio.get_event_loop().time()

                try:
                    await self._events.on_try(attempt_number)
                    return await delegate(prompt, **kwargs)
                except BaseException as error:
                    if isinstance(error, tuple(self._retryable_errors)):
                        await self._events.on_retryable_error(error, attempt_number)
                        await self._on_retryable_error(error)
                    raise
                finally:
                    call_end = asyncio.get_event_loop().time()
                    call_times.append(call_end - call_start)

            async def execute_with_retry() -> LLMOutput[
                TOutput, TJsonModel, THistoryEntry
            ]:
                nonlocal attempt_number
                try:
                    async for a in AsyncRetrying(
                        stop=stop_after_attempt(self._max_retries),
                        wait=wait_exponential_jitter(max=self._max_retry_wait),
                        reraise=True,
                        retry=retry_if_exception_type(tuple(self._retryable_errors)),
                    ):
                        with a:
                            attempt_number += 1
                            return await attempt()
                except BaseException as error:
                    if not isinstance(error, tuple(self._retryable_errors)):
                        raise

                raise RetriesExhaustedError(name, self._max_retries)

            start = asyncio.get_event_loop().time()
            result = await execute_with_retry()
            end = asyncio.get_event_loop().time()

            result.metrics.retry = LLMRetryMetrics(
                num_retries=attempt_number - 1,
                total_time=end - start,
                call_times=call_times,
            )

            await self._events.on_success(result.metrics)

            return result

        return invoke

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/services/errors.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""Base LLM related errors."""


class RetriesExhaustedError(RuntimeError):
    """Retries exhausted error."""

    def __init__(self, name: str, num_retries: int) -> None:
        """Init method definition."""
        super().__init__(
            f"Operation '{name}' failed - {num_retries} retries exhausted."
        )


class FailedToGenerateValidJsonError(RuntimeError):
    """Failed to create valid JSON error."""

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/services/history_extractor.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""LLM cache-interactor module."""

from abc import ABC, abstractmethod
from collections.abc import Sequence
from typing import Generic

from fnllm.types.generics import THistoryEntry, TOutput


class HistoryExtractor(ABC, Generic[TOutput, THistoryEntry]):
    """History extractor base class."""

    @abstractmethod
    def extract_history(
        self,
        history: Sequence[THistoryEntry] | None,
        output: TOutput,
    ) -> list[THistoryEntry]:
        """Extract history from a response."""

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/services/variable_injector.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""Generic LLM variables replacing module."""

from string import Template
from typing import TypeVar, cast

from fnllm.types import PromptVariables

TInput = TypeVar("TInput")


class VariableInjector:
    """An variables replacing LLM."""

    def inject_variables(
        self, prompt: TInput, variables: PromptVariables | None
    ) -> TInput:
        """Call the LLM."""
        parsed_prompt = prompt

        if isinstance(parsed_prompt, str) and variables:
            parsed_prompt = Template(parsed_prompt).substitute(**variables)

        return cast(TInput, parsed_prompt)

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/services/json.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""Base protocol for decorator services."""

import json
from abc import ABC, abstractmethod
from collections.abc import Awaitable, Callable
from typing import Generic, cast

import pydantic
from json_repair import repair_json
from typing_extensions import Unpack

from fnllm.services.errors import FailedToGenerateValidJsonError
from fnllm.types.generics import (
    JSON,
    THistoryEntry,
    TInput,
    TJsonModel,
    TModelParameters,
    TOutput,
)
from fnllm.types.io import LLMInput, LLMOutput

from .decorator import LLMDecorator


class JsonHandler(Generic[TOutput, THistoryEntry]):
    """A json-handling strategy."""

    def __init__(
        self,
        requester: LLMDecorator[TOutput, THistoryEntry] | None = None,
        receiver: LLMDecorator[TOutput, THistoryEntry] | None = None,
    ):
        """Create a new JsonHandler."""
        self._requester = requester
        self._receiver = receiver

    @property
    def requester(self) -> LLMDecorator[TOutput, THistoryEntry] | None:
        """The inner decorator."""
        return self._requester

    @property
    def receiver(self) -> LLMDecorator[TOutput, THistoryEntry] | None:
        """The inner decorator."""
        return self._receiver


class BaseJsonDecorator(
    ABC, LLMDecorator[TOutput, THistoryEntry], Generic[TInput, TOutput, THistoryEntry]
):
    """An OpenAI JSON parsing LLM."""

    def decorate(
        self,
        delegate: Callable[
            ..., Awaitable[LLMOutput[TOutput, TJsonModel, THistoryEntry]]
        ],
    ) -> Callable[..., Awaitable[LLMOutput[TOutput, TJsonModel, THistoryEntry]]]:
        """Decorate the delegate with the JSON functionality."""
        this = self

        async def invoke(
            prompt: TInput,
            **kwargs: Unpack[LLMInput[TJsonModel, THistoryEntry, TModelParameters]],
        ) -> LLMOutput[TOutput, TJsonModel, THistoryEntry]:
            if kwargs.get("json_model") is not None or kwargs.get("json"):
                return await this.invoke_json(delegate, prompt, kwargs)
            return await delegate(prompt, **kwargs)

        return invoke

    @abstractmethod
    async def invoke_json(
        self,
        delegate: Callable[
            ..., Awaitable[LLMOutput[TOutput, TJsonModel, THistoryEntry]]
        ],
        prompt: TInput,
        kwargs: LLMInput[TJsonModel, THistoryEntry, TModelParameters],
    ) -> LLMOutput[TOutput, TJsonModel, THistoryEntry]:
        """Invoke the JSON decorator."""


class JsonMarshaler(ABC, Generic[TOutput, THistoryEntry]):
    """Marshal JSON data into and out of output payloads."""

    @abstractmethod
    def inject_json_string(
        self,
        json_string: str | None,
        output: LLMOutput[TOutput, TJsonModel, THistoryEntry],
    ) -> LLMOutput[TOutput, TJsonModel, THistoryEntry]:
        """Inject the JSON string into the output."""

    @abstractmethod
    def extract_json_string(
        self, output: LLMOutput[TOutput, TJsonModel, THistoryEntry]
    ) -> str | None:
        """Extract the JSON string from the output."""


class JsonRequester(
    BaseJsonDecorator[TInput, TOutput, THistoryEntry],
    Generic[TInput, TOutput, THistoryEntry, TModelParameters],
):
    """A decorator for handling JSON output. This implementation should be the outer decorator."""

    @abstractmethod
    def rewrite_args(
        self,
        prompt: TInput,
        kwargs: LLMInput[TJsonModel, THistoryEntry, TModelParameters],
    ) -> tuple[TInput, LLMInput[TJsonModel, THistoryEntry, TModelParameters]]:
        """Rewrite the input prompt and arguments.."""

    async def invoke_json(
        self,
        delegate: Callable[
            ..., Awaitable[LLMOutput[TOutput, TJsonModel, THistoryEntry]]
        ],
        prompt: TInput,
        kwargs: LLMInput[TJsonModel, THistoryEntry, TModelParameters],
    ) -> LLMOutput[TOutput, TJsonModel, THistoryEntry]:
        """Invoke the JSON decorator."""
        prompt, kwargs = self.rewrite_args(prompt, kwargs)
        return await delegate(prompt, **kwargs)


class JsonReceiver(
    BaseJsonDecorator[TInput, TOutput, THistoryEntry],
    Generic[TInput, TOutput, THistoryEntry, TModelParameters],
):
    """A decorator for handling JSON output. This implementation should be the outer decorator."""

    def __init__(
        self,
        marshaler: JsonMarshaler[TOutput, THistoryEntry],
        max_retries: int,
    ):
        """Create a new JsonReceiver."""
        self._marshaler = marshaler
        self._max_retries = max_retries

    async def invoke_json(
        self,
        delegate: Callable[
            ..., Awaitable[LLMOutput[TOutput, TJsonModel, THistoryEntry]]
        ],
        prompt: TInput,
        kwargs: LLMInput[TJsonModel, THistoryEntry, TModelParameters],
    ) -> LLMOutput[TOutput, TJsonModel, THistoryEntry]:
        """Invoke the JSON decorator."""
        error: FailedToGenerateValidJsonError | None = None
        name = kwargs.get("name", "")
        for attempt in range(self._max_retries + 1):
            try:
                if attempt > 0:
                    kwargs["name"] = f"{name}-(retry {attempt})"
                return await self.try_receive_json(delegate, prompt, kwargs)
            except FailedToGenerateValidJsonError as e:
                error = e

        raise FailedToGenerateValidJsonError from error

    async def try_receive_json(
        self,
        delegate: Callable[
            ..., Awaitable[LLMOutput[TOutput, TJsonModel, THistoryEntry]]
        ],
        prompt: TInput,
        kwargs: LLMInput[TJsonModel, THistoryEntry, TModelParameters],
    ) -> LLMOutput[TOutput, TJsonModel, THistoryEntry]:
        """Invoke the JSON decorator."""
        json_model = kwargs.get("json_model")
        result = await delegate(prompt, **kwargs)
        json_string = self._marshaler.extract_json_string(result)
        raw_json = self._parse_json_string(json_string)
        model = self._read_model_from_json(raw_json, json_model)
        self._marshaler.inject_json_string(json_string, result)
        result.raw_json = raw_json
        result.parsed_json = model
        return result

    def _parse_json_string(
        self,
        value: str | None,
    ) -> JSON | None:
        try:
            return json.loads(value) if value else None
        except json.JSONDecodeError as err:
            msg = f"JSON response is not a valid JSON, response={value}."
            raise FailedToGenerateValidJsonError(msg) from err

    def _read_model_from_json(
        self,
        value: JSON | None,
        json_model: type[TJsonModel] | None,
    ) -> TJsonModel | None:
        if value is None or json_model is None:
            return None
        try:
            return json_model.model_validate(value)
        except pydantic.ValidationError as err:
            msg = f"JSON response does not match the expected model, response={value}."
            raise FailedToGenerateValidJsonError(msg) from err


class LooseModeJsonReceiver(
    JsonReceiver[TInput, TOutput, THistoryEntry, TModelParameters],
    Generic[TInput, TOutput, THistoryEntry, TModelParameters],
):
    """A decorator for handling JSON output. This implementation should be the outer decorator."""

    async def try_receive_json(
        self,
        delegate: Callable[
            ..., Awaitable[LLMOutput[TOutput, TJsonModel, THistoryEntry]]
        ],
        prompt: TInput,
        kwargs: LLMInput[TJsonModel, THistoryEntry, TModelParameters],
    ) -> LLMOutput[TOutput, TJsonModel, THistoryEntry]:
        """Invoke the JSON decorator."""
        json_model = kwargs.get("json_model")

        result = await delegate(prompt, **kwargs)
        json_string = self._marshaler.extract_json_string(result)
        try:
            raw_json = self._parse_json_string(json_string)
            model = self._read_model_from_json(raw_json, json_model)
        except FailedToGenerateValidJsonError as err:
            # A 'None' value would not have thrown an error
            json_string = cast(str, json_string)
            try:
                (
                    json_string,
                    raw_json,
                    model,
                ) = await self._try_recovering_malformed_json(
                    err, json_string, prompt, kwargs
                )
                if raw_json is not None:
                    self._marshaler.inject_json_string(json_string, result)
                    result.raw_json = raw_json
                    result.parsed_json = model
                    return result

                # The recovery didn't work, raise the error
                raise
            except BaseException:  # noqa BLE001
                # The recovery didn't work, raise the error
                raise FailedToGenerateValidJsonError from err
        else:
            self._marshaler.inject_json_string(json_string, result)
            result.raw_json = raw_json
            result.parsed_json = model
            return result

    def _parse_json_string(
        self,
        value: str | None,
    ) -> JSON | None:
        try:
            return json.loads(value) if value else None
        except json.JSONDecodeError as err:
            msg = f"JSON response is not a valid JSON, response={value}."
            raise FailedToGenerateValidJsonError(msg) from err

    def _read_model_from_json(
        self,
        value: JSON | None,
        json_model: type[TJsonModel] | None,
    ) -> TJsonModel | None:
        if value is None or json_model is None:
            return None
        try:
            return json_model.model_validate(value)
        except pydantic.ValidationError as err:
            msg = f"JSON response does not match the expected model, response={value}."
            raise FailedToGenerateValidJsonError(msg) from err

    async def _try_recovering_malformed_json(
        self,
        err: FailedToGenerateValidJsonError,
        json_string: str,
        prompt: TInput,
        kwargs: LLMInput[TJsonModel, THistoryEntry, TModelParameters],
    ) -> tuple[str | None, JSON | None, TJsonModel | None]:
        """Try to recover from a bad JSON error. Null JSON = unable to recover."""
        json_string = cast(str, repair_json(json_string, skip_json_loads=True))
        json_object = cast(JSON, json.loads(json_string)) if json_string else None
        json_model = kwargs.get("json_model")
        model_instance = json_model.model_validate(json_object) if json_model else None
        return (json_string, json_object, model_instance)

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/openai/types/embeddings/io.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""OpenAI embeddings input/output types."""

from typing import TypeAlias

from fnllm.openai.types.aliases import OpenAIEmbeddingModel
from fnllm.types.generalized import EmbeddingsLLMInput, EmbeddingsLLMOutput
from fnllm.types.metrics import LLMUsageMetrics

OpenAIEmbeddingsInput: TypeAlias = EmbeddingsLLMInput
"""Main input type for OpenAI embeddings."""


class OpenAIEmbeddingsOutput(EmbeddingsLLMOutput):
    """OpenAI embeddings completion output."""

    raw_input: OpenAIEmbeddingsInput | None
    """Raw input that resulted in this output."""

    raw_output: list[OpenAIEmbeddingModel]
    """Raw embeddings output from OpenAI."""

    usage: LLMUsageMetrics | None
    """Usage statistics for the embeddings request."""

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/openai/types/embeddings/__init__.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""OpenAI embeddings types."""

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/openai/types/embeddings/parameters.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""OpenAI embeddings parameters types."""

from typing import Literal

from typing_extensions import NotRequired, TypedDict


class OpenAIEmbeddingsParameters(TypedDict):
    """OpenAI allowed embeddings parameters."""

    model: NotRequired[str]

    dimensions: NotRequired[int]

    encoding_format: NotRequired[Literal["float", "base64"]]

    user: NotRequired[str]

    timeout: NotRequired[float]

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/openai/types/chat/io.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""OpenAI input/output types."""

from collections.abc import AsyncIterable, Awaitable, Callable
from typing import ClassVar, TypeAlias

from pydantic import BaseModel, ConfigDict, Field

from fnllm.openai.types.aliases import (
    OpenAIChatCompletionMessageModel,
    OpenAIChatCompletionMessageParam,
)
from fnllm.types.generalized import ChatLLMOutput
from fnllm.types.metrics import LLMUsageMetrics

OpenAIChatMessageInput: TypeAlias = OpenAIChatCompletionMessageParam
"""OpenAI chat message input."""

OpenAIChatHistoryEntry: TypeAlias = OpenAIChatCompletionMessageParam
"""OpenAI chat history entry."""

OpenAIChatCompletionInput: TypeAlias = str | OpenAIChatMessageInput | None
"""Main input type for OpenAI completions."""


class OpenAIChatOutput(ChatLLMOutput):
    """OpenAI chat completion output."""

    raw_input: OpenAIChatMessageInput | None
    """Raw input that resulted in this output."""

    raw_output: OpenAIChatCompletionMessageModel
    """Raw output message from OpenAI."""

    usage: LLMUsageMetrics | None
    """Usage statistics for the completion request."""


class OpenAIStreamingChatOutput(BaseModel, arbitrary_types_allowed=True):
    """Async iterable chat content."""

    model_config: ClassVar[ConfigDict] = ConfigDict(arbitrary_types_allowed=True)

    raw_input: OpenAIChatMessageInput | None = Field(
        default=None, description="Raw input that resulted in this output."
    )

    usage: LLMUsageMetrics | None = Field(
        default=None,
        description="Usage statistics for the completion request.\nThis will only be available after the stream is complete, if the LLM has been configured to emit usage.",
    )

    content: AsyncIterable[str | None] = Field(exclude=True)

    close: Callable[[], Awaitable[None]] = Field(
        description="Close the underlying iterator", exclude=True
    )

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/openai/types/chat/__init__.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""OpenAI chat types."""

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/openai/types/chat/parameters.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""OpenAI chat parameters types."""

from collections.abc import Iterable
from typing import Literal

from typing_extensions import NotRequired, TypedDict

from fnllm.openai.types.aliases import (
    OpenAIChatCompletionToolChoiceOptionParam,
    OpenAIChatCompletionToolParam,
    OpenAIChatModel,
    OpenAIFunctionCallCreateParam,
    OpenAIFunctionCreateParam,
    OpenAIResponseFormatCreateParam,
)


#
# Note: streaming options have been removed from this class to avoid downstream tying issues.
# OpenAI streaming should be handled with a StreamingLLM, not additional client-side parameters.
#
class OpenAIChatParameters(TypedDict):
    """OpenAI allowed chat parameters."""

    model: NotRequired[str | OpenAIChatModel]

    frequency_penalty: NotRequired[float | None]

    function_call: NotRequired[OpenAIFunctionCallCreateParam]

    functions: NotRequired[Iterable[OpenAIFunctionCreateParam]]

    logit_bias: NotRequired[dict[str, int] | None]

    logprobs: NotRequired[bool | None]

    max_tokens: NotRequired[int | None]

    n: NotRequired[int | None]

    parallel_tool_calls: NotRequired[bool]

    presence_penalty: NotRequired[float | None]

    response_format: NotRequired[OpenAIResponseFormatCreateParam]

    seed: NotRequired[int | None]

    service_tier: NotRequired[Literal["auto", "default"] | None]

    stop: NotRequired[str | None | list[str]]

    temperature: NotRequired[float | None]

    tool_choice: NotRequired[OpenAIChatCompletionToolChoiceOptionParam]

    tools: NotRequired[Iterable[OpenAIChatCompletionToolParam]]

    top_logprobs: NotRequired[int | None]

    top_p: NotRequired[float | None]

    user: NotRequired[str]

    timeout: NotRequired[float | None]

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/openai/types/client.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""OpenAI client types."""

from typing import Literal, Protocol, TypeAlias, overload, runtime_checkable

from openai import AsyncAzureOpenAI, AsyncOpenAI
from typing_extensions import Unpack

from fnllm.openai.types.chat.io import (
    OpenAIChatCompletionInput,
    OpenAIChatHistoryEntry,
    OpenAIChatOutput,
    OpenAIStreamingChatOutput,
)
from fnllm.openai.types.chat.parameters import OpenAIChatParameters
from fnllm.openai.types.embeddings.io import (
    OpenAIEmbeddingsInput,
    OpenAIEmbeddingsOutput,
)
from fnllm.openai.types.embeddings.parameters import OpenAIEmbeddingsParameters
from fnllm.types.generics import TJsonModel
from fnllm.types.io import LLMInput, LLMOutput
from fnllm.types.protocol import LLM

OpenAIClient = AsyncOpenAI | AsyncAzureOpenAI
"""Allowed OpenAI client types."""

OpenAITextChatLLM: TypeAlias = LLM[
    OpenAIChatCompletionInput,
    OpenAIChatOutput,
    OpenAIChatHistoryEntry,
    OpenAIChatParameters,
]
"""Alias for the fully typed OpenAIChatLLM instance."""

OpenAIStreamingChatLLM: TypeAlias = LLM[
    OpenAIChatCompletionInput,
    OpenAIStreamingChatOutput,
    OpenAIChatHistoryEntry,
    OpenAIChatParameters,
]

OpenAIEmbeddingsLLM: TypeAlias = LLM[
    OpenAIEmbeddingsInput, OpenAIEmbeddingsOutput, None, OpenAIEmbeddingsParameters
]
"""Alias for the fully typed OpenAIEmbeddingsLLM instance."""


@runtime_checkable
class OpenAIChatLLM(Protocol):
    """Protocol for the OpenAI chat LLM."""

    @overload
    async def __call__(
        self,
        prompt: OpenAIChatCompletionInput,
        *,
        stream: Literal[True],
        **kwargs: Unpack[
            LLMInput[TJsonModel, OpenAIChatHistoryEntry, OpenAIChatParameters]
        ],
    ) -> LLMOutput[OpenAIStreamingChatOutput, TJsonModel, OpenAIChatHistoryEntry]: ...

    @overload
    async def __call__(
        self,
        prompt: OpenAIChatCompletionInput,
        *,
        stream: Literal[False] | None = None,
        **kwargs: Unpack[
            LLMInput[TJsonModel, OpenAIChatHistoryEntry, OpenAIChatParameters]
        ],
    ) -> LLMOutput[OpenAIChatOutput, TJsonModel, OpenAIChatHistoryEntry]: ...

    def child(self, name: str) -> "OpenAIChatLLM":
        """Create a child LLM."""
        ...

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/openai/types/__init__.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""OpenAI specific types."""

from .aliases import (
    OpenAIChatCompletionAssistantMessageParam,
    OpenAIChatCompletionFunctionMessageParam,
    OpenAIChatCompletionMessageModel,
    OpenAIChatCompletionMessageParam,
    OpenAIChatCompletionMessageToolCallModel,
    OpenAIChatCompletionMessageToolCallParam,
    OpenAIChatCompletionModel,
    OpenAIChatCompletionStreamOptionsParam,
    OpenAIChatCompletionSystemMessageParam,
    OpenAIChatCompletionToolChoiceOptionParam,
    OpenAIChatCompletionToolMessageParam,
    OpenAIChatCompletionToolParam,
    OpenAIChatCompletionUserMessageParam,
    OpenAIChatModel,
    OpenAIChoiceModel,
    OpenAICompletionUsageModel,
    OpenAICreateEmbeddingResponseModel,
    OpenAIEmbeddingModel,
    OpenAIEmbeddingUsageModel,
    OpenAIFunctionCallCreateParam,
    OpenAIFunctionCallModel,
    OpenAIFunctionCallParam,
    OpenAIFunctionCreateParam,
    OpenAIFunctionDefinitionParam,
    OpenAIFunctionModel,
    OpenAIFunctionParam,
    OpenAIResponseFormatCreateParam,
)
from .chat.io import (
    OpenAIChatCompletionInput,
    OpenAIChatHistoryEntry,
    OpenAIChatMessageInput,
    OpenAIChatOutput,
    OpenAIStreamingChatOutput,
)
from .chat.parameters import OpenAIChatParameters
from .client import (
    OpenAIChatLLM,
    OpenAIClient,
    OpenAIEmbeddingsLLM,
    OpenAITextChatLLM,
)
from .embeddings.io import OpenAIEmbeddingsInput, OpenAIEmbeddingsOutput
from .embeddings.parameters import OpenAIEmbeddingsParameters

__all__ = [
    "OpenAIChatCompletionAssistantMessageParam",
    "OpenAIChatCompletionFunctionMessageParam",
    "OpenAIChatCompletionInput",
    "OpenAIChatCompletionMessageModel",
    "OpenAIChatCompletionMessageParam",
    "OpenAIChatCompletionMessageToolCallModel",
    "OpenAIChatCompletionMessageToolCallParam",
    "OpenAIChatCompletionModel",
    "OpenAIChatCompletionStreamOptionsParam",
    "OpenAIChatCompletionSystemMessageParam",
    "OpenAIChatCompletionToolChoiceOptionParam",
    "OpenAIChatCompletionToolMessageParam",
    "OpenAIChatCompletionToolParam",
    "OpenAIChatCompletionUserMessageParam",
    "OpenAIChatHistoryEntry",
    "OpenAIChatLLM",
    "OpenAIChatMessageInput",
    "OpenAIChatModel",
    "OpenAIChatOutput",
    "OpenAIChatParameters",
    "OpenAIChoiceModel",
    "OpenAIClient",
    "OpenAICompletionUsageModel",
    "OpenAICreateEmbeddingResponseModel",
    "OpenAIEmbeddingModel",
    "OpenAIEmbeddingUsageModel",
    "OpenAIEmbeddingsInput",
    "OpenAIEmbeddingsLLM",
    "OpenAIEmbeddingsOutput",
    "OpenAIEmbeddingsParameters",
    "OpenAIFunctionCallCreateParam",
    "OpenAIFunctionCallModel",
    "OpenAIFunctionCallParam",
    "OpenAIFunctionCreateParam",
    "OpenAIFunctionDefinitionParam",
    "OpenAIFunctionModel",
    "OpenAIFunctionParam",
    "OpenAIResponseFormatCreateParam",
    "OpenAIStreamingChatOutput",
    "OpenAITextChatLLM",
]

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/openai/types/aliases.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""OpenAI aliases types."""

from collections.abc import Sequence
from typing import Literal, TypeAlias

from openai.types.chat import (
    ChatCompletion as ChatCompletionModel,
)
from openai.types.chat import (
    ChatCompletionMessage as ChatCompletionMessageModel,
)
from openai.types.chat.chat_completion import Choice as ChoiceModel
from openai.types.chat.chat_completion_assistant_message_param import (
    FunctionCall as FunctionCallParam,
)
from openai.types.chat.chat_completion_function_message_param import (
    ChatCompletionFunctionMessageParam,
)
from openai.types.chat.chat_completion_message import FunctionCall as FunctionCallModel
from openai.types.chat.chat_completion_message_tool_call import (
    ChatCompletionMessageToolCall as ChatCompletionMessageToolCallModel,
)
from openai.types.chat.chat_completion_message_tool_call import (
    Function as FunctionModel,
)
from openai.types.chat.chat_completion_message_tool_call_param import (
    ChatCompletionMessageToolCallParam,
)
from openai.types.chat.chat_completion_message_tool_call_param import (
    Function as FunctionParam,
)
from openai.types.chat.chat_completion_stream_options_param import (
    ChatCompletionStreamOptionsParam,
)
from openai.types.chat.chat_completion_system_message_param import (
    ChatCompletionSystemMessageParam,
)
from openai.types.chat.chat_completion_tool_choice_option_param import (
    ChatCompletionToolChoiceOptionParam,
)
from openai.types.chat.chat_completion_tool_message_param import (
    ChatCompletionToolMessageParam,
)
from openai.types.chat.chat_completion_tool_param import ChatCompletionToolParam
from openai.types.chat.chat_completion_user_message_param import (
    ChatCompletionUserMessageParam,
)
from openai.types.chat.completion_create_params import (
    Function as FunctionCreateParam,
)
from openai.types.chat.completion_create_params import (
    FunctionCall as FunctionCallCreateParam,
)
from openai.types.chat.completion_create_params import (
    ResponseFormat as ResponseFormatCreateParam,
)
from openai.types.chat_model import ChatModel
from openai.types.completion_usage import CompletionUsage as CompletionUsageModel
from openai.types.create_embedding_response import (
    CreateEmbeddingResponse as CreateEmbeddingResponseModel,
)
from openai.types.create_embedding_response import Usage as EmbeddingUsageModel
from openai.types.embedding import Embedding as EmbeddingModel
from openai.types.shared_params.function_definition import (
    FunctionDefinition as FunctionDefinitionParam,
)
from typing_extensions import Required, TypedDict

OpenAIChatModel: TypeAlias = ChatModel
"""Alias for the ChatModel (available model types)."""

OpenAICompletionUsageModel: TypeAlias = CompletionUsageModel
"""Alias for the CompletionUsage (base model)."""

OpenAIChatCompletionStreamOptionsParam: TypeAlias = ChatCompletionStreamOptionsParam
"""Alias for the ChatCompletionStreamOptionsParam (param)."""

OpenAIChatCompletionModel: TypeAlias = ChatCompletionModel
"""Alias for the ChatCompletion (base model)."""

OpenAIChatCompletionMessageModel: TypeAlias = ChatCompletionMessageModel
"""Alias for the ChatCompletionMessage (base model)."""

OpenAIChoiceModel: TypeAlias = ChoiceModel
"""Alias for the Choice (base model)."""

OpenAIFunctionModel: TypeAlias = FunctionModel
"""Alias for the Function (base model)."""

OpenAIFunctionParam: TypeAlias = FunctionParam
"""Alias for the Function (param)."""

OpenAIFunctionCreateParam: TypeAlias = FunctionCreateParam
"""Alias for the Function (create param)."""

OpenAIFunctionCallModel: TypeAlias = FunctionCallModel
"""Alias for the FunctionCall (base model)."""

OpenAIFunctionCallParam: TypeAlias = FunctionCallParam
"""Alias for the FunctionCall (param)."""

OpenAIFunctionCallCreateParam: TypeAlias = FunctionCallCreateParam
"""Alias for the FunctionCall (create param)."""

OpenAIFunctionDefinitionParam: TypeAlias = FunctionDefinitionParam
"""Alias for the FunctionDefinition (param)."""

OpenAIResponseFormatCreateParam: TypeAlias = ResponseFormatCreateParam
"""Alias for the ResponseFormatCreateParam (create param)."""

OpenAIChatCompletionMessageToolCallModel: TypeAlias = ChatCompletionMessageToolCallModel
"""Alias for the ChatCompletionMessageToolCall (base model)."""

OpenAIChatCompletionToolParam: TypeAlias = ChatCompletionToolParam
"""Alias for the ChatCompletionToolParam (param)."""

OpenAIChatCompletionMessageToolCallParam: TypeAlias = ChatCompletionMessageToolCallParam
"""Alias for the ChatCompletionMessageToolCallParam (param)."""

OpenAIChatCompletionToolChoiceOptionParam: TypeAlias = (
    ChatCompletionToolChoiceOptionParam
)
"""Alias for the ChatCompletionToolChoiceOptionParam (param)."""


# NOTE:
# This is done to avoid using an Iterator for the `tool_calls`,
# which when combined with pydantic will result in a generator that
# can only be iterated once
class _ChatCompletionAssistantMessageParam(TypedDict, total=False):
    """Shadow ChatCompletionAssistantMessageParam from OpenAI."""

    role: Required[Literal["assistant"]]
    """The role of the messages author, in this case `assistant`."""

    content: str | None
    """The contents of the assistant message.

    Required unless `tool_calls` or `function_call` is specified.
    """

    function_call: OpenAIFunctionCallParam | None
    """Deprecated and replaced by `tool_calls`.

    The name and arguments of a function that should be called, as generated by the
    model.
    """

    name: str
    """An optional name for the participant.

    Provides the model information to differentiate between participants of the same
    role.
    """

    tool_calls: Sequence[OpenAIChatCompletionMessageToolCallParam]
    """The tool calls generated by the model, such as function calls."""


OpenAIChatCompletionSystemMessageParam: TypeAlias = ChatCompletionSystemMessageParam
"""Alias for the ChatCompletionSystemMessageParam (param)."""

OpenAIChatCompletionUserMessageParam: TypeAlias = ChatCompletionUserMessageParam
"""Alias for the ChatCompletionUserMessageParam (param)."""

OpenAIChatCompletionAssistantMessageParam: TypeAlias = (
    _ChatCompletionAssistantMessageParam
)
"""Alias for the ChatCompletionAssistantMessageParam (param)."""

OpenAIChatCompletionToolMessageParam: TypeAlias = ChatCompletionToolMessageParam
"""Alias for the ChatCompletionToolMessageParam (param)."""

OpenAIChatCompletionFunctionMessageParam: TypeAlias = ChatCompletionFunctionMessageParam
"""Alias for the ChatCompletionFunctionMessageParam (param)."""

OpenAIChatCompletionMessageParam: TypeAlias = (
    OpenAIChatCompletionSystemMessageParam
    | OpenAIChatCompletionUserMessageParam
    | OpenAIChatCompletionAssistantMessageParam
    | OpenAIChatCompletionToolMessageParam
    | OpenAIChatCompletionFunctionMessageParam
)
"""OpenAI possible chat completion message types (param)."""

OpenAICreateEmbeddingResponseModel: TypeAlias = CreateEmbeddingResponseModel
"""Alias for the CreateEmbeddingResponse (base model)."""

OpenAIEmbeddingModel: TypeAlias = EmbeddingModel
"""Alias for the Embedding (base model)."""

OpenAIEmbeddingUsageModel: TypeAlias = EmbeddingUsageModel
"""Alias for the EmbeddingUsage (base model)."""

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/openai/llm/chat_text.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""The chat-based LLM implementation."""

from collections.abc import Iterator
from typing import Any, cast

from openai.types.chat.chat_completion_message_param import ChatCompletionMessageParam
from typing_extensions import Unpack

from fnllm.base.base import BaseLLM
from fnllm.events.base import LLMEvents
from fnllm.openai.types.aliases import OpenAIChatCompletionModel, OpenAIChatModel
from fnllm.openai.types.chat.io import (
    OpenAIChatCompletionInput,
    OpenAIChatHistoryEntry,
    OpenAIChatOutput,
)
from fnllm.openai.types.chat.parameters import OpenAIChatParameters
from fnllm.openai.types.client import OpenAIClient
from fnllm.services.cache_interactor import CacheInteractor
from fnllm.services.json import JsonHandler
from fnllm.services.rate_limiter import RateLimiter
from fnllm.services.retryer import Retryer
from fnllm.services.variable_injector import VariableInjector
from fnllm.types.generics import TJsonModel
from fnllm.types.io import LLMInput
from fnllm.types.metrics import LLMUsageMetrics

from .services.history_extractor import OpenAIHistoryExtractor
from .services.usage_extractor import OpenAIUsageExtractor
from .utils import build_chat_messages


# Create a logger for OpenAIChatLLM
import logging
openai_logger = logging.getLogger("OpenAIChatLLM")
openai_logger.setLevel(logging.DEBUG)
openai_handler = logging.FileHandler("/tmp/graphrag/openai_chat_llm.log")
openai_handler.setLevel(logging.DEBUG)
openai_logger.addHandler(openai_handler)

class OpenAITextChatLLMImpl(
    BaseLLM[
        OpenAIChatCompletionInput,
        OpenAIChatOutput,
        OpenAIChatHistoryEntry,
        OpenAIChatParameters,
    ]
):
    """A chat-based LLM."""

    def __init__(
        self,
        client: OpenAIClient,
        model: str | OpenAIChatModel,
        cache: CacheInteractor,
        *,
        usage_extractor: OpenAIUsageExtractor[OpenAIChatOutput] | None = None,
        history_extractor: OpenAIHistoryExtractor | None = None,
        variable_injector: VariableInjector | None = None,
        rate_limiter: RateLimiter[
            OpenAIChatCompletionInput,
            OpenAIChatOutput,
            OpenAIChatHistoryEntry,
            OpenAIChatParameters,
        ]
        | None = None,
        retryer: Retryer[
            OpenAIChatCompletionInput,
            OpenAIChatOutput,
            OpenAIChatHistoryEntry,
            OpenAIChatParameters,
        ]
        | None = None,
        model_parameters: OpenAIChatParameters | None = None,
        events: LLMEvents | None = None,
        json_handler: JsonHandler[OpenAIChatOutput, OpenAIChatHistoryEntry]
        | None = None,
    ):
        """Create a new OpenAIChatLLM."""
        super().__init__(
            events=events,
            usage_extractor=usage_extractor,
            history_extractor=history_extractor,
            variable_injector=variable_injector,
            retryer=retryer,
            rate_limiter=rate_limiter,
            json_handler=json_handler,
        )

        self._client = client
        self._model = model
        self._global_model_parameters = model_parameters or {}
        self._cache = cache

    def child(self, name: str) -> Any:
        """Create a child LLM."""
        return OpenAITextChatLLMImpl(
            self._client,
            self._model,
            self._cache.child(name),
            events=self.events,
            usage_extractor=cast(
                OpenAIUsageExtractor[OpenAIChatOutput], self._usage_extractor
            ),
            history_extractor=cast(OpenAIHistoryExtractor, self._history_extractor),
            variable_injector=self._variable_injector,
            rate_limiter=self._rate_limiter,
            retryer=self._retryer,
            model_parameters=self._global_model_parameters,
            json_handler=self._json_handler,
        )

    def _build_completion_parameters(
        self, local_parameters: OpenAIChatParameters | None
    ) -> OpenAIChatParameters:
        params: OpenAIChatParameters = {
            "model": self._model,
            **self._global_model_parameters,
            **(local_parameters or {}),
        }

        return params

    async def _call_completion_or_cache(
        self,
        name: str | None,
        *,
        messages: list[OpenAIChatHistoryEntry],
        parameters: OpenAIChatParameters,
        bypass_cache: bool,
    ) -> OpenAIChatCompletionModel:
        # TODO: check if we need to remove max_tokens and n from the keys
        return await self._cache.get_or_insert(
            lambda: self._client.chat.completions.create(
                messages=cast(Iterator[ChatCompletionMessageParam], messages),
                **parameters,
            ),
            prefix=f"chat_{name}" if name else "chat",
            key_data={"messages": messages, "parameters": parameters},
            name=name,
            json_model=OpenAIChatCompletionModel,
            bypass_cache=bypass_cache,
        )

    async def _execute_llm(
        self,
        prompt: OpenAIChatCompletionInput,
        **kwargs: Unpack[
            LLMInput[TJsonModel, OpenAIChatHistoryEntry, OpenAIChatParameters]
        ],
    ) -> OpenAIChatOutput:
        name = kwargs.get("name")
        history = kwargs.get("history", [])
        bypass_cache = kwargs.get("bypass_cache", False)
        local_model_parameters = kwargs.get("model_parameters")
        messages, prompt_message = build_chat_messages(prompt, history)
        completion_parameters = self._build_completion_parameters(
            local_model_parameters
        )

        completion = await self._call_completion_or_cache(
            name,
            messages=messages,
            parameters=completion_parameters,
            bypass_cache=bypass_cache,
        )

        response = completion.choices[0].message
 
        
        output =  OpenAIChatOutput(
            raw_input=prompt_message,
            raw_output=response,
            content=response.content,
            usage=LLMUsageMetrics(
                input_tokens=completion.usage.prompt_tokens,
                output_tokens=completion.usage.completion_tokens,
            )
            if completion.usage
            else None,
        )
        if True or "community" in str(prompt).lower():
            
            
            #dataiku_logger.info("DataikuChatLLM: Prompt: %s", prompt_content)
            openai_logger.info("OpenAIChatLLM: history=%s", history)
            openai_logger.debug("OpenAIChatLLM: Prompt: %s", prompt)
            openai_logger.debug("OpenAIChatLLM: Response Content: %s", response.content)


        return output

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/openai/llm/features/__init__.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""Package with OpenAI specific features to be used to wrap base LLM protocol interfaces."""

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/openai/llm/features/tools_parsing.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""LLM tools parsing module for OpenAI."""

from collections.abc import Sequence

import pydantic
from typing_extensions import Unpack

from fnllm.openai.llm.utils import llm_tools_to_param
from fnllm.openai.types.aliases import (
    OpenAIChatCompletionMessageModel,
    OpenAIChatCompletionMessageToolCallModel,
)
from fnllm.openai.types.chat.io import (
    OpenAIChatCompletionInput,
    OpenAIChatHistoryEntry,
    OpenAIChatOutput,
)
from fnllm.openai.types.chat.parameters import OpenAIChatParameters
from fnllm.tools import LLMTool
from fnllm.tools.errors import ToolInvalidArgumentsError, ToolNotFoundError
from fnllm.types.generics import TJsonModel
from fnllm.types.io import LLMInput, LLMOutput
from fnllm.types.protocol import LLM


class OpenAIParseToolsLLM(
    LLM[
        OpenAIChatCompletionInput,
        OpenAIChatOutput,
        OpenAIChatHistoryEntry,
        OpenAIChatParameters,
    ],
):
    """An OpenAI tools parsing LLM."""

    def __init__(
        self,
        delegate: LLM[
            OpenAIChatCompletionInput,
            OpenAIChatOutput,
            OpenAIChatHistoryEntry,
            OpenAIChatParameters,
        ],
    ):
        """Create a new OpenAIParseToolsLLM."""
        self._delegate = delegate

    def child(self, name: str) -> "OpenAIParseToolsLLM":
        """Create a child LLM (with child cache)."""
        return OpenAIParseToolsLLM(self._delegate.child(name))

    def _add_tools_to_parameters(
        self,
        parameters: LLMInput[TJsonModel, OpenAIChatHistoryEntry, OpenAIChatParameters],
        tools: Sequence[type[LLMTool]],
    ) -> LLMInput[TJsonModel, OpenAIChatHistoryEntry, OpenAIChatParameters]:
        new_parameters = parameters.copy()

        new_parameters["model_parameters"] = new_parameters.get("model_parameters", {})
        new_parameters["model_parameters"]["tools"] = llm_tools_to_param(tools)

        return new_parameters

    def _parse_arguments(
        self,
        tool_call: OpenAIChatCompletionMessageToolCallModel,
        *,
        json_model: type[LLMTool],
        raw_output: OpenAIChatCompletionMessageModel,
    ) -> LLMTool:
        try:
            return json_model.model_validate_json(tool_call.function.arguments)
        except pydantic.ValidationError as err:
            raise ToolInvalidArgumentsError(
                raw_output,
                tool_call=tool_call,
                expected_tool=json_model,
                validation_error=err,
            ) from err

    def _parse_tool_calls(
        self,
        raw_output: OpenAIChatCompletionMessageModel,
        *,
        tools: Sequence[type[LLMTool]],
    ) -> list[LLMTool]:
        result = []
        tool_calls = raw_output.tool_calls or []

        for call in tool_calls:
            tool = LLMTool.find_tool(tools, call.function.name)

            if not tool:
                raise ToolNotFoundError(raw_output, tool_call=call)

            parsed_json = self._parse_arguments(
                call, json_model=tool, raw_output=raw_output
            )

            parsed_json.__raw_arguments_json__ = call.function.arguments
            parsed_json.call_id = call.id

            result.append(parsed_json)

        return result

    async def __call__(
        self,
        prompt: OpenAIChatCompletionInput,
        **kwargs: Unpack[
            LLMInput[TJsonModel, OpenAIChatHistoryEntry, OpenAIChatParameters]
        ],
    ) -> LLMOutput[OpenAIChatOutput, TJsonModel, OpenAIChatHistoryEntry]:
        """Call the LLM."""
        tools = kwargs.get("tools", [])

        if not tools:
            return await self._delegate(prompt, **kwargs)

        completion_parameters = self._add_tools_to_parameters(kwargs, tools)

        result = await self._delegate(prompt, **completion_parameters)

        result.tool_calls = self._parse_tool_calls(
            result.output.raw_output,
            tools=tools,
        )

        return result

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/openai/llm/__init__.py ==================
# Copyright (c) 2024 Microsoft Corporation.


"""OpenAI LLM implementations."""

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/openai/llm/chat.py ==================
# Copyright (c) 2024 Microsoft Corporation.
"""OpenAI Chat LLM."""

from typing import Any, Literal, overload

from typing_extensions import Unpack

from fnllm.openai.types.chat.io import (
    OpenAIChatCompletionInput,
    OpenAIChatHistoryEntry,
    OpenAIChatOutput,
    OpenAIStreamingChatOutput,
)
from fnllm.openai.types.chat.parameters import OpenAIChatParameters
from fnllm.openai.types.client import (
    OpenAIChatLLM,
    OpenAIStreamingChatLLM,
    OpenAITextChatLLM,
)
from fnllm.types.generics import TJsonModel
from fnllm.types.io import LLMInput, LLMOutput


class OpenAIChatLLMImpl(OpenAIChatLLM):
    """The OpenAIChatLLM Facade."""

    def __init__(
        self,
        *,
        text_chat_llm: OpenAITextChatLLM,
        streaming_chat_llm: OpenAIStreamingChatLLM,
    ):
        """Create a new OpenAI Chat Facade."""
        self._text_chat_llm = text_chat_llm
        self._streaming_chat_llm = streaming_chat_llm

    def child(self, name: str) -> "OpenAIChatLLMImpl":
        """Create a child LLM (with child cache)."""
        return OpenAIChatLLMImpl(
            text_chat_llm=self._text_chat_llm.child(name),
            streaming_chat_llm=self._streaming_chat_llm.child(name),
        )

    @overload
    async def __call__(
        self,
        prompt: OpenAIChatCompletionInput,
        *,
        stream: Literal[True],
        **kwargs: Unpack[
            LLMInput[TJsonModel, OpenAIChatHistoryEntry, OpenAIChatParameters]
        ],
    ) -> LLMOutput[OpenAIStreamingChatOutput, TJsonModel, OpenAIChatHistoryEntry]: ...

    @overload
    async def __call__(
        self,
        prompt: OpenAIChatCompletionInput,
        *,
        stream: Literal[False] | None = None,
        **kwargs: Unpack[
            LLMInput[TJsonModel, OpenAIChatHistoryEntry, OpenAIChatParameters]
        ],
    ) -> LLMOutput[OpenAIChatOutput, TJsonModel, OpenAIChatHistoryEntry]: ...

    async def __call__(
        self,
        prompt: OpenAIChatCompletionInput,
        *,
        stream: bool | None = None,
        **kwargs: Unpack[
            LLMInput[TJsonModel, OpenAIChatHistoryEntry, OpenAIChatParameters]
        ],
    ) -> LLMOutput[
        Any | OpenAIStreamingChatOutput | OpenAIChatOutput,
        TJsonModel,
        OpenAIChatHistoryEntry,
    ]:
        """Invoke the streaming chat output."""
        if stream:
            return await self._streaming_chat_llm(prompt, **kwargs)

        return await self._text_chat_llm(prompt, **kwargs)

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/openai/llm/embeddings.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""The EmbeddingsLLM class."""

from typing import cast

from typing_extensions import Unpack

from fnllm.base.base import BaseLLM
from fnllm.events.base import LLMEvents
from fnllm.openai.types.aliases import OpenAICreateEmbeddingResponseModel
from fnllm.openai.types.client import OpenAIClient
from fnllm.openai.types.embeddings.io import (
    OpenAIEmbeddingsInput,
    OpenAIEmbeddingsOutput,
)
from fnllm.openai.types.embeddings.parameters import OpenAIEmbeddingsParameters
from fnllm.services.cache_interactor import CacheInteractor
from fnllm.services.rate_limiter import RateLimiter
from fnllm.services.retryer import Retryer
from fnllm.services.variable_injector import VariableInjector
from fnllm.types.io import LLMInput
from fnllm.types.metrics import LLMUsageMetrics

from .services.usage_extractor import OpenAIUsageExtractor


class OpenAIEmbeddingsLLMImpl(
    BaseLLM[
        OpenAIEmbeddingsInput, OpenAIEmbeddingsOutput, None, OpenAIEmbeddingsParameters
    ],
):
    """A text-embedding generator LLM."""

    def __init__(
        self,
        client: OpenAIClient,
        model: str,
        cache: CacheInteractor,
        *,
        usage_extractor: OpenAIUsageExtractor[OpenAIEmbeddingsOutput] | None = None,
        variable_injector: VariableInjector | None = None,
        rate_limiter: RateLimiter[
            OpenAIEmbeddingsInput,
            OpenAIEmbeddingsOutput,
            None,
            OpenAIEmbeddingsParameters,
        ]
        | None = None,
        retryer: Retryer[
            OpenAIEmbeddingsInput,
            OpenAIEmbeddingsOutput,
            None,
            OpenAIEmbeddingsParameters,
        ]
        | None = None,
        model_parameters: OpenAIEmbeddingsParameters | None = None,
        events: LLMEvents | None = None,
    ):
        """Create a new OpenAIEmbeddingsLLM."""
        super().__init__(
            events=events,
            usage_extractor=usage_extractor,
            variable_injector=variable_injector,
            rate_limiter=rate_limiter,
            retryer=retryer,
        )

        self._client = client
        self._model = model
        self._cache = cache
        self._global_model_parameters = model_parameters or {}

    def child(self, name: str) -> "OpenAIEmbeddingsLLMImpl":
        """Create a child LLM."""
        return OpenAIEmbeddingsLLMImpl(
            self._client,
            self._model,
            self._cache.child(name),
            usage_extractor=cast(
                OpenAIUsageExtractor[OpenAIEmbeddingsOutput], self._usage_extractor
            ),
            variable_injector=self._variable_injector,
            rate_limiter=self._rate_limiter,
            retryer=self._retryer,
            model_parameters=self._global_model_parameters,
            events=self._events,
        )

    def _build_embeddings_parameters(
        self, local_parameters: OpenAIEmbeddingsParameters | None
    ) -> OpenAIEmbeddingsParameters:
        params: OpenAIEmbeddingsParameters = {
            "model": self._model,
            **self._global_model_parameters,
            **(local_parameters or {}),
        }

        return params

    async def _call_embeddings_or_cache(
        self,
        name: str | None,
        *,
        prompt: OpenAIEmbeddingsInput,
        parameters: OpenAIEmbeddingsParameters,
        bypass_cache: bool,
    ) -> OpenAICreateEmbeddingResponseModel:
        # TODO: check if we need to remove max_tokens and n from the keys
        return await self._cache.get_or_insert(
            lambda: self._client.embeddings.create(
                input=prompt,
                **parameters,
            ),
            prefix=f"embeddings_{name}" if name else "embeddings",
            key_data={"input": prompt, "parameters": parameters},
            name=name,
            bypass_cache=bypass_cache,
            json_model=OpenAICreateEmbeddingResponseModel,
        )

    async def _execute_llm(
        self, prompt: OpenAIEmbeddingsInput, **kwargs: Unpack[LLMInput]
    ) -> OpenAIEmbeddingsOutput:
        name = kwargs.get("name")
        local_model_parameters = kwargs.get("model_parameters")
        bypass_cache = kwargs.get("bypass_cache", False)

        embeddings_parameters = self._build_embeddings_parameters(
            local_model_parameters
        )

        response = await self._call_embeddings_or_cache(
            name,
            prompt=prompt,
            parameters=embeddings_parameters,
            bypass_cache=bypass_cache,
        )

        return OpenAIEmbeddingsOutput(
            raw_input=prompt,
            raw_output=response.data,
            embeddings=[d.embedding for d in response.data],
            usage=LLMUsageMetrics(
                input_tokens=response.usage.prompt_tokens,
            )
            if response.usage
            else None,
        )

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/openai/llm/utils.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""OpenAI parsing utilities."""

from collections.abc import Iterable, Sequence
from typing import cast

from fnllm.openai.types.aliases import (
    OpenAIChatCompletionAssistantMessageParam,
    OpenAIChatCompletionMessageModel,
    OpenAIChatCompletionMessageToolCallModel,
    OpenAIChatCompletionMessageToolCallParam,
    OpenAIChatCompletionToolParam,
    OpenAIChatCompletionUserMessageParam,
    OpenAIFunctionCallModel,
    OpenAIFunctionCallParam,
    OpenAIFunctionDefinitionParam,
    OpenAIFunctionModel,
    OpenAIFunctionParam,
)
from fnllm.openai.types.chat.io import OpenAIChatCompletionInput, OpenAIChatHistoryEntry
from fnllm.tools.base import LLMTool


def function_call_to_param(
    func: OpenAIFunctionCallModel | None,
) -> OpenAIFunctionCallParam | None:
    """Parses FunctionCall base model to the equivalent typed dict."""
    if not func:
        return None

    return OpenAIFunctionCallParam(
        arguments=func.arguments,
        name=func.name,
    )


def function_to_param(func: OpenAIFunctionModel) -> OpenAIFunctionParam:
    """Parses Function base model to the equivalent typed dict."""
    return OpenAIFunctionParam(arguments=func.arguments, name=func.name)


def tool_calls_to_params(
    tools: list[OpenAIChatCompletionMessageToolCallModel] | None,
) -> Sequence[OpenAIChatCompletionMessageToolCallParam] | None:
    """Parses a list of ChatCompletionMessageToolCall base model to the equivalent typed dict."""
    if not tools:
        return None

    return [
        OpenAIChatCompletionMessageToolCallParam(
            id=tool.id, function=function_to_param(tool.function), type=tool.type
        )
        for tool in tools
    ]


def llm_tool_to_param(tool: type[LLMTool]) -> OpenAIFunctionDefinitionParam:
    """Parses a class that implements LLMTool to the equivalent typed dict."""
    return OpenAIFunctionDefinitionParam(
        name=tool.get_name(),
        description=tool.get_description(),
        parameters=tool.get_parameters_schema(),
    )


def llm_tools_to_param(
    tools: Sequence[type[LLMTool]],
) -> Iterable[OpenAIChatCompletionToolParam]:
    """Parses a list of classes that implements LLMTool to the equivalent typed dicts."""
    return [
        OpenAIChatCompletionToolParam(
            function=llm_tool_to_param(tool),
            type="function",
        )
        for tool in tools
    ]


def chat_completion_message_to_param(
    message: OpenAIChatCompletionMessageModel,
) -> OpenAIChatCompletionAssistantMessageParam:
    """Parses ChatCompletionMessage base model to the equivalent typed dict."""
    param = OpenAIChatCompletionAssistantMessageParam(
        role=message.role, content=message.content
    )

    function_call = function_call_to_param(message.function_call)

    if function_call:
        param["function_call"] = function_call

    tool_calls = tool_calls_to_params(message.tool_calls)

    if tool_calls:
        param["tool_calls"] = tool_calls

    return param


def build_chat_messages(
    prompt: OpenAIChatCompletionInput,
    history: Sequence[OpenAIChatHistoryEntry],
) -> tuple[list[OpenAIChatHistoryEntry], OpenAIChatHistoryEntry]:
    """Builds a chat history list from the prompt and existing history, along with the prompt message."""
    if isinstance(prompt, str):
        prompt = OpenAIChatCompletionUserMessageParam(
            content=prompt,
            role="user",
        )
    messages = [*history]
    if prompt is not None:
        messages.append(prompt)
    return messages, cast(OpenAIChatHistoryEntry, prompt)

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/openai/llm/chat_streaming.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""The chat-based LLM implementation."""

import traceback
from collections.abc import AsyncIterator, Callable, Iterator
from typing import TypeAlias, cast

from openai import AsyncStream
from openai.types.chat import ChatCompletionChunk
from openai.types.chat.chat_completion_message_param import ChatCompletionMessageParam
from typing_extensions import Unpack

from fnllm.base.base import BaseLLM
from fnllm.events.base import LLMEvents
from fnllm.openai.types.aliases import OpenAIChatModel
from fnllm.openai.types.chat.io import (
    OpenAIChatCompletionInput,
    OpenAIChatHistoryEntry,
    OpenAIStreamingChatOutput,
)
from fnllm.openai.types.chat.parameters import OpenAIChatParameters
from fnllm.openai.types.client import OpenAIClient
from fnllm.services.rate_limiter import RateLimiter
from fnllm.services.retryer import Retryer
from fnllm.services.variable_injector import VariableInjector
from fnllm.types import LLMMetrics, LLMUsageMetrics
from fnllm.types.generics import TJsonModel
from fnllm.types.io import LLMInput

from .utils import build_chat_messages

ChunkStream: TypeAlias = AsyncStream[ChatCompletionChunk]


class OpenAIStreamingChatLLMImpl(
    BaseLLM[
        OpenAIChatCompletionInput,
        OpenAIStreamingChatOutput,
        OpenAIChatHistoryEntry,
        OpenAIChatParameters,
    ]
):
    """A chat-based LLM."""

    def __init__(
        self,
        client: OpenAIClient,
        model: str | OpenAIChatModel,
        *,
        variable_injector: VariableInjector | None = None,
        rate_limiter: RateLimiter[
            OpenAIChatCompletionInput,
            OpenAIStreamingChatOutput,
            OpenAIChatHistoryEntry,
            OpenAIChatParameters,
        ]
        | None = None,
        retryer: Retryer[
            OpenAIChatCompletionInput,
            OpenAIStreamingChatOutput,
            OpenAIChatHistoryEntry,
            OpenAIChatParameters,
        ]
        | None = None,
        emit_usage: bool = False,
        model_parameters: OpenAIChatParameters | None = None,
        events: LLMEvents | None = None,
    ):
        """Create a new OpenAIChatLLM."""
        super().__init__(
            events=events,
            variable_injector=variable_injector,
            rate_limiter=rate_limiter,
            retryer=retryer,
        )

        self._client = client
        self._model = model
        self._emit_usage = emit_usage
        self._global_model_parameters = model_parameters or {}

    def child(self, name: str) -> "OpenAIStreamingChatLLMImpl":
        """Create a child LLM."""
        return self

    def _build_completion_parameters(
        self, local_parameters: OpenAIChatParameters | None
    ) -> OpenAIChatParameters:
        params: OpenAIChatParameters = {
            "model": self._model,
            **self._global_model_parameters,
            **(local_parameters or {}),
        }

        return params

    async def _execute_llm(
        self,
        prompt: OpenAIChatCompletionInput,
        **kwargs: Unpack[
            LLMInput[TJsonModel, OpenAIChatHistoryEntry, OpenAIChatParameters]
        ],
    ) -> OpenAIStreamingChatOutput:
        history = kwargs.get("history", [])
        local_model_parameters = kwargs.get("model_parameters")
        messages, prompt_message = build_chat_messages(prompt, history)
        completion_parameters = self._build_completion_parameters(
            local_model_parameters
        )

        completion_kwargs = {**completion_parameters, "stream": True}
        if self._emit_usage:
            completion_kwargs["stream_options"] = {"include_usage": True}

        completion: ChunkStream = await self._client.chat.completions.create(
            messages=cast(Iterator[ChatCompletionMessageParam], messages),
            **completion_kwargs,
        )

        iterator = StreamingChatIterator(chunks=completion, events=self._events)
        result = OpenAIStreamingChatOutput(
            raw_input=prompt_message,
            content=iterator.iterator,
            close=iterator.close,
        )

        def handle_usage(usage: LLMUsageMetrics) -> None:
            result.usage = usage

        iterator.on_usage(handle_usage)
        return result


class StreamingChatIterator:
    """A streaming llm response iterator."""

    def __init__(
        self,
        chunks: ChunkStream,
        events: LLMEvents,
    ):
        """Create a new Response."""
        self._chunks = chunks
        self._events = events
        self._iterator = self.__stream__()

    def on_usage(self, cb: Callable[[LLMUsageMetrics], None]) -> None:
        """Handle usage events."""
        self._on_usage = cb

    async def __stream__(self) -> AsyncIterator[str | None]:
        """Read chunks from the stream."""
        usage = LLMUsageMetrics()
        try:
            async for chunk in self._chunks:
                # Note: this is only emitted _just_ prior to the stream completing.
                if chunk.usage:
                    usage = LLMUsageMetrics(
                        input_tokens=chunk.usage.prompt_tokens,
                        output_tokens=chunk.usage.completion_tokens,
                    )
                    if self._on_usage:
                        self._on_usage(usage)
                    await self._events.on_usage(usage)

                if chunk.choices and len(chunk.choices) > 0:
                    yield chunk.choices[0].delta.content
        except BaseException as e:
            stack_trace = traceback.format_exc()
            await self._events.on_error(e, stack_trace, {"streaming": True})
            raise

        self._on_usage = None
        await self._events.on_success(
            LLMMetrics(
                estimated_input_tokens=usage.input_tokens,
                usage=usage,
            )
        )

    @property
    def iterator(self) -> AsyncIterator[str | None]:
        """Return the content."""
        return self._iterator

    async def close(self) -> None:
        """Close the stream."""
        await self._chunks.close()

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/openai/llm/services/usage_extractor.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""LLM metrics parsing module for OpenAI."""

from typing import Generic, TypeVar

from fnllm.openai.types.chat.io import OpenAIChatOutput
from fnllm.openai.types.embeddings.io import OpenAIEmbeddingsOutput
from fnllm.services.usage_extractor import UsageExtractor
from fnllm.types.metrics import LLMUsageMetrics

TOutputWithUsageMetrics = TypeVar(
    "TOutputWithUsageMetrics", OpenAIChatOutput, OpenAIEmbeddingsOutput
)
"""Represents the support output types for usage metrics parsing."""


class OpenAIUsageExtractor(
    UsageExtractor[TOutputWithUsageMetrics],
    Generic[TOutputWithUsageMetrics],
):
    """An OpenAI usage metrics parsing LLM."""

    def extract_usage(self, output: TOutputWithUsageMetrics) -> LLMUsageMetrics:
        """Extract the LLM Usage from an OpenAI response."""
        return output.usage or LLMUsageMetrics()

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/openai/llm/services/rate_limiter.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""Rate limiting LLM implementation for OpenAI."""

import json
from typing import Final, Generic

from openai import APIConnectionError, InternalServerError, RateLimitError
from tiktoken import Encoding

from fnllm.events.base import LLMEvents
from fnllm.limiting import Limiter
from fnllm.openai.llm.utils import llm_tools_to_param
from fnllm.services.rate_limiter import RateLimiter
from fnllm.types.generics import (
    THistoryEntry,
    TInput,
    TJsonModel,
    TModelParameters,
    TOutput,
)
from fnllm.types.io import LLMInput

OPENAI_RETRYABLE_ERRORS: Final[list[type[Exception]]] = [
    RateLimitError,
    APIConnectionError,
    InternalServerError,
]


class OpenAIRateLimiter(
    RateLimiter[TInput, TOutput, THistoryEntry, TModelParameters],
    Generic[TInput, TOutput, THistoryEntry, TModelParameters],
):
    """A base class to rate limit the LLM."""

    def __init__(
        self,
        limiter: Limiter,
        encoder: Encoding,
        *,
        events: LLMEvents | None = None,
    ):
        """Create a new BaseRateLimitLLM."""
        super().__init__(
            limiter,
            events=events,
        )
        self._encoding = encoder

    def _estimate_request_tokens(
        self,
        prompt: TInput,
        kwargs: LLMInput[TJsonModel, THistoryEntry, TModelParameters],
    ) -> int:
        history = kwargs.get("history", [])
        tools = llm_tools_to_param(kwargs.get("tools", []))

        return sum(
            len(self._encoding.encode(json.dumps(entry)))
            for entry in (*history, *tools, prompt)
        )

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/openai/llm/services/__init__.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""Package with OpenAI specific features to be used to wrap base LLM protocol interfaces."""

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/openai/llm/services/retryer.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""Rate limiting LLM implementation for OpenAI."""

import asyncio
from typing import Final, Generic

from openai import APIConnectionError, InternalServerError, RateLimitError

from fnllm.events.base import LLMEvents
from fnllm.services.retryer import Retryer
from fnllm.types.generics import THistoryEntry, TInput, TModelParameters, TOutput

OPENAI_RETRYABLE_ERRORS: Final[list[type[Exception]]] = [
    RateLimitError,
    APIConnectionError,
    InternalServerError,
]


class OpenAIRetryer(
    Retryer[TInput, TOutput, THistoryEntry, TModelParameters],
    Generic[TInput, TOutput, THistoryEntry, TModelParameters],
):
    """A base class to rate limit the LLM."""

    def __init__(
        self,
        *,
        tag: str = "OpenAIRetryingLLM",
        max_retries: int = 10,
        max_retry_wait: float = 10,
        sleep_on_rate_limit_recommendation: bool = False,
        events: LLMEvents | None = None,
    ):
        """Create a new BaseRateLimitLLM."""
        super().__init__(
            retryable_errors=OPENAI_RETRYABLE_ERRORS,
            tag=tag,
            max_retries=max_retries,
            max_retry_wait=max_retry_wait,
            events=events,
        )
        self._sleep_on_rate_limit_recommendation = sleep_on_rate_limit_recommendation

    async def _on_retryable_error(self, error: BaseException) -> None:
        sleep_recommendation = self._extract_sleep_recommendation(error)
        if sleep_recommendation > 0:
            await asyncio.sleep(sleep_recommendation)

    def _extract_sleep_recommendation(self, error: BaseException) -> float:
        """Extract the sleep time value from a RateLimitError. This is usually only available in Azure."""
        please_retry_after_msg: Final = "Rate limit is exceeded. Try again in "

        if not self._sleep_on_rate_limit_recommendation:
            return 0

        error_str = str(error)

        if (
            not isinstance(error, RateLimitError)
            or please_retry_after_msg not in error_str
        ):
            return 0

        # could be second or seconds
        return int(error_str.split(please_retry_after_msg)[1].split(" second")[0])

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/openai/llm/services/history_extractor.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""LLM history tracking module for OpenAI."""

from collections.abc import Sequence

from fnllm.openai.llm.utils import chat_completion_message_to_param
from fnllm.openai.types.chat.io import OpenAIChatHistoryEntry, OpenAIChatOutput
from fnllm.services.history_extractor import HistoryExtractor


class OpenAIHistoryExtractor(
    HistoryExtractor[OpenAIChatOutput, OpenAIChatHistoryEntry]
):
    """An OpenAI history-tracking LLM."""

    def extract_history(
        self,
        history: Sequence[OpenAIChatHistoryEntry] | None,
        output: OpenAIChatOutput,
    ) -> list[OpenAIChatHistoryEntry]:
        """Call the LLM."""
        result = [*history] if history else []

        if output.raw_input is not None:
            result.append(output.raw_input)

        result.append(chat_completion_message_to_param(output.raw_output))

        return result

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/openai/llm/services/json.py ==================
# Copyright (c) 2024 Microsoft Corporation.
"""OpenAI JSON Handler."""

from fnllm.config.json_strategy import JsonStrategy
from fnllm.openai.types.chat.io import (
    OpenAIChatCompletionInput,
    OpenAIChatHistoryEntry,
    OpenAIChatOutput,
)
from fnllm.openai.types.chat.parameters import OpenAIChatParameters
from fnllm.services.json import (
    JsonHandler,
    JsonMarshaler,
    JsonReceiver,
    JsonRequester,
    LooseModeJsonReceiver,
)
from fnllm.types.generics import TJsonModel
from fnllm.types.io import LLMInput, LLMOutput


def create_json_handler(
    strategy: JsonStrategy,
    max_retries: int,
) -> JsonHandler[OpenAIChatOutput, OpenAIChatHistoryEntry]:
    """Create a JSON handler for OpenAI."""
    marshaler = OpenAIJsonMarshaler()
    match strategy:
        case JsonStrategy.LOOSE:
            return JsonHandler(None, LooseModeJsonReceiver(marshaler, max_retries))
        case JsonStrategy.VALID:
            return JsonHandler(
                OpenAIJsonRequester(), JsonReceiver(marshaler, max_retries)
            )
        case JsonStrategy.STRUCTURED:
            raise NotImplementedError


class OpenAIJsonMarshaler(JsonMarshaler[OpenAIChatOutput, OpenAIChatHistoryEntry]):
    """An OpenAI JSON marshaler."""

    def inject_json_string(
        self,
        json_string: str | None,
        output: LLMOutput[OpenAIChatOutput, TJsonModel, OpenAIChatHistoryEntry],
    ) -> LLMOutput[OpenAIChatOutput, TJsonModel, OpenAIChatHistoryEntry]:
        """Inject the JSON string into the output."""
        output.output.content = json_string
        return output

    def extract_json_string(
        self, output: LLMOutput[OpenAIChatOutput, TJsonModel, OpenAIChatHistoryEntry]
    ) -> str | None:
        """Extract the JSON string from the output."""
        return output.output.content


class OpenAIJsonRequester(
    JsonRequester[
        OpenAIChatCompletionInput,
        OpenAIChatOutput,
        OpenAIChatHistoryEntry,
        OpenAIChatParameters,
    ]
):
    """An OpenAI JSON requester."""

    def rewrite_args(
        self,
        prompt: OpenAIChatCompletionInput,
        kwargs: LLMInput[TJsonModel, OpenAIChatHistoryEntry, OpenAIChatParameters],
    ) -> tuple[
        OpenAIChatCompletionInput,
        LLMInput[TJsonModel, OpenAIChatHistoryEntry, OpenAIChatParameters],
    ]:
        """Rewrite the input prompt and arguments.."""
        kwargs["model_parameters"] = self._enable_oai_json_mode(
            kwargs.get("model_parameters", {})
        )
        return prompt, kwargs

    def _enable_oai_json_mode(
        self, parameters: OpenAIChatParameters
    ) -> OpenAIChatParameters:
        result: OpenAIChatParameters = parameters.copy()
        result["response_format"] = {"type": "json_object"}
        return result

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/openai/config.py ==================
# Copyright (c) 2024 Microsoft Corporation.


"""OpenAI Configuration class definition."""

from typing import Annotated, Literal

from pydantic import Field

from fnllm.config import Config
from fnllm.openai.types.chat.parameters import OpenAIChatParameters
from fnllm.openai.types.embeddings.parameters import OpenAIEmbeddingsParameters


class CommonOpenAIConfig(Config, frozen=True, extra="allow", protected_namespaces=()):
    """Common configuration parameters between Azure OpenAI and Public OpenAI."""

    azure: bool = Field(default=False, description="Whether to use Azure OpenAI.")

    api_key: str | None = Field(default=None, description="The OpenAI API key.")

    track_stream_usage: bool = Field(
        default=False, description="Whether to emit stream usage."
    )

    organization: str | None = Field(
        default=None, description="The OpenAI organization."
    )

    timeout: float | None = Field(default=None, description="The request timeout.")

    model: str = Field(default="", description="The OpenAI model to use.")

    encoding: str = Field(default="cl100k_base", description="The encoding model.")

    chat_parameters: OpenAIChatParameters = Field(
        default_factory=dict,
        description="Global chat parameters to be used across calls.",
    )

    embeddings_parameters: OpenAIEmbeddingsParameters = Field(
        default_factory=dict,
        description="Global embeddings parameters to be used across calls.",
    )

    sleep_on_rate_limit_recommendation: bool = Field(
        default=False,
        description="Whether to sleep on rate limit recommendation.",
    )


class PublicOpenAIConfig(
    CommonOpenAIConfig, frozen=True, extra="allow", protected_namespaces=()
):
    """Public OpenAI configuration definition."""

    azure: Literal[False] = Field(
        default=False, description="Whether to use Azure OpenAI."
    )

    base_url: str | None = Field(default=None, description="The OpenAI API base URL.")


class AzureOpenAIConfig(
    CommonOpenAIConfig, frozen=True, extra="allow", protected_namespaces=()
):
    """Azure OpenAI configuration definition."""

    azure: Literal[True] = Field(
        default=True, description="Whether to use Azure OpenAI."
    )

    endpoint: str = Field(description="The OpenAI API endpoint.")

    deployment: str | None = Field(
        default=None, description="The Azure deployment name."
    )

    api_version: str | None = Field(description="The OpenAI API version.")

    cognitive_services_endpoint: str = Field(
        default="https://cognitiveservices.azure.com/.default",
        description="The Azure Cognitive Services endpoint.",
    )


OpenAIConfig = Annotated[
    PublicOpenAIConfig | AzureOpenAIConfig, Field(discriminator="azure")
]
"""OpenAI configuration definition."""

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/openai/roles.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""OpenAI specific roles."""

from collections.abc import Sequence
from string import Template
from typing import Any, Final

from fnllm.openai.types.aliases import (
    OpenAIChatCompletionAssistantMessageParam,
    OpenAIChatCompletionFunctionMessageParam,
    OpenAIChatCompletionMessageToolCallParam,
    OpenAIChatCompletionSystemMessageParam,
    OpenAIChatCompletionToolMessageParam,
    OpenAIChatCompletionUserMessageParam,
    OpenAIFunctionCallParam,
)


class _OpenAIBaseRole:
    """OpenAI base class for roles."""

    role = "user"

    def _substitute_template(
        self, value: str | Template, variables: dict[str, Any] | None
    ) -> str:
        if isinstance(value, Template):
            return value.substitute(**(variables or {}))

        if variables:
            return Template(value).substitute(**variables)

        return value

    def __str__(self) -> str:
        """String representation of the role."""
        return self.role

    def __hash__(self) -> int:
        """Hash representation of the role."""
        return hash(self.role)


class _OpenAISystemRole(_OpenAIBaseRole):
    """OpenAI system role."""

    role: Final = "system"

    def message(
        self,
        content: str | Template,
        *,
        name: str | None = None,
        variables: dict[str, Any] | None = None,
    ) -> OpenAIChatCompletionSystemMessageParam:
        """Create a message for the given role."""
        msg = OpenAIChatCompletionSystemMessageParam(
            content=self._substitute_template(content, variables), role=self.role
        )

        if name is not None:
            msg["name"] = name

        return msg


class _OpenAIUserRole(_OpenAIBaseRole):
    """OpenAI user role."""

    role: Final = "user"

    def message(
        self,
        content: str | Template,
        *,
        name: str | None = None,
        variables: dict[str, Any] | None = None,
    ) -> OpenAIChatCompletionUserMessageParam:
        """Create a message for the given role."""
        msg = OpenAIChatCompletionUserMessageParam(
            content=self._substitute_template(content, variables), role=self.role
        )

        if name is not None:
            msg["name"] = name

        return msg


class _OpenAIAssistantRole(_OpenAIBaseRole):
    """OpenAI assistant role."""

    role: Final = "assistant"

    def message(
        self,
        content: str | Template | None = None,
        *,
        tool_calls: Sequence[OpenAIChatCompletionMessageToolCallParam] | None = None,
        function_call: OpenAIFunctionCallParam | None = None,
        name: str | None = None,
        variables: dict[str, Any] | None = None,
    ) -> OpenAIChatCompletionAssistantMessageParam:
        """Create a message for the given role."""
        msg = OpenAIChatCompletionAssistantMessageParam(
            content=self._substitute_template(content, variables)
            if content is not None
            else None,
            role=self.role,
        )

        if tool_calls is not None:
            msg["tool_calls"] = tool_calls

        if function_call is not None:
            msg["function_call"] = function_call

        if name is not None:
            msg["name"] = name

        return msg


class _OpenAIToolRole(_OpenAIBaseRole):
    """OpenAI tool role."""

    role: Final = "tool"

    def message(
        self,
        content: str | Template,
        tool_call_id: str,
        *,
        variables: dict[str, Any] | None = None,
    ) -> OpenAIChatCompletionToolMessageParam:
        """Create a message for the given role."""
        return OpenAIChatCompletionToolMessageParam(
            content=self._substitute_template(content, variables),
            tool_call_id=tool_call_id,
            role=self.role,
        )


class _OpenAIFunctionRole(_OpenAIBaseRole):
    """OpenAI function role."""

    role: Final = "function"

    def message(
        self,
        content: str | Template,
        name: str,
        *,
        variables: dict[str, Any] | None = None,
    ) -> OpenAIChatCompletionFunctionMessageParam:
        """Create a message for the given role."""
        return OpenAIChatCompletionFunctionMessageParam(
            content=self._substitute_template(content, variables),
            name=name,
            role=self.role,
        )


class OpenAIChatRole:
    """OpenAI chat roles."""

    System: Final = _OpenAISystemRole()

    User: Final = _OpenAIUserRole()

    Assistant: Final = _OpenAIAssistantRole()

    Tool: Final = _OpenAIToolRole()

    Function: Final = _OpenAIFunctionRole()

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/openai/__init__.py ==================
# Copyright (c) 2024 Microsoft Corporation.


"""OpenAI LLM implementations."""

from .config import AzureOpenAIConfig, OpenAIConfig, PublicOpenAIConfig
from .factories import (
    create_openai_chat_llm,
    create_openai_client,
    create_openai_embeddings_llm,
)
from .roles import OpenAIChatRole
from .types.client import (
    OpenAIClient,
    OpenAIEmbeddingsLLM,
    OpenAIStreamingChatLLM,
    OpenAITextChatLLM,
)

# TODO: include type aliases?
__all__ = [
    "AzureOpenAIConfig",
    "OpenAIChatRole",
    "OpenAIClient",
    "OpenAIConfig",
    "OpenAIConfig",
    "OpenAIEmbeddingsLLM",
    "OpenAIStreamingChatLLM",
    "OpenAITextChatLLM",
    "PublicOpenAIConfig",
    "create_openai_chat_llm",
    "create_openai_client",
    "create_openai_embeddings_llm",
]

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/openai/factories/client.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""Create OpenAI client instance."""

from typing import cast

from openai import AsyncAzureOpenAI, AsyncOpenAI

from fnllm.openai.config import AzureOpenAIConfig, OpenAIConfig, PublicOpenAIConfig
from fnllm.openai.types.client import OpenAIClient


def create_openai_client(config: OpenAIConfig) -> OpenAIClient:
    """Create a new OpenAI client instance."""
    if config.azure:
        from azure.identity import DefaultAzureCredential, get_bearer_token_provider

        config = cast(AzureOpenAIConfig, config)

        token_provider = (
            get_bearer_token_provider(
                DefaultAzureCredential(), config.cognitive_services_endpoint
            )
            if not config.api_key
            else None
        )

        return AsyncAzureOpenAI(
            api_key=config.api_key,
            azure_ad_token_provider=token_provider,
            organization=config.organization,
            # Azure-Specifics
            api_version=config.api_version,
            azure_endpoint=config.endpoint,
            azure_deployment=config.deployment,
            timeout=config.timeout,
            max_retries=0,
        )

    config = cast(PublicOpenAIConfig, config)

    return AsyncOpenAI(
        api_key=config.api_key,
        base_url=config.base_url,
        organization=config.organization,
        timeout=config.timeout,
        max_retries=0,
    )

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/openai/factories/__init__.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""Methods to create OpenAI instances."""

from .chat import create_openai_chat_llm
from .client import create_openai_client
from .embeddings import create_openai_embeddings_llm

__all__ = [
    "create_openai_chat_llm",
    "create_openai_client",
    "create_openai_embeddings_llm",
]

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/openai/factories/chat.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""Factory functions for creating OpenAI LLMs."""

from fnllm.caching.base import Cache
from fnllm.events.base import LLMEvents
from fnllm.limiting.base import Limiter
from fnllm.openai.config import OpenAIConfig
from fnllm.openai.llm.chat import OpenAIChatLLMImpl
from fnllm.openai.llm.chat_streaming import OpenAIStreamingChatLLMImpl
from fnllm.openai.llm.chat_text import OpenAITextChatLLMImpl
from fnllm.openai.llm.features.tools_parsing import OpenAIParseToolsLLM
from fnllm.openai.llm.services.history_extractor import OpenAIHistoryExtractor
from fnllm.openai.llm.services.json import create_json_handler
from fnllm.openai.llm.services.usage_extractor import OpenAIUsageExtractor
from fnllm.openai.types.client import (
    OpenAIChatLLM,
    OpenAIClient,
    OpenAIStreamingChatLLM,
    OpenAITextChatLLM,
)
from fnllm.services.cache_interactor import CacheInteractor
from fnllm.services.variable_injector import VariableInjector

from .client import create_openai_client
from .utils import create_limiter, create_rate_limiter, create_retryer


def create_openai_chat_llm(
    config: OpenAIConfig,
    *,
    client: OpenAIClient | None = None,
    cache: Cache | None = None,
    cache_interactor: CacheInteractor | None = None,
    events: LLMEvents | None = None,
) -> OpenAIChatLLM:
    """Create an OpenAI chat LLM."""
    if client is None:
        client = create_openai_client(config)

    limiter = create_limiter(config)

    text_chat_llm = _create_openai_text_chat_llm(
        client=client,
        config=config,
        cache=cache,
        cache_interactor=cache_interactor,
        events=events,
        limiter=limiter,
    )
    streaming_chat_llm = _create_openai_streaming_chat_llm(
        client=client,
        config=config,
        events=events,
        limiter=limiter,
    )
    return OpenAIChatLLMImpl(
        text_chat_llm=text_chat_llm,
        streaming_chat_llm=streaming_chat_llm,
    )


def _create_openai_text_chat_llm(
    *,
    client: OpenAIClient,
    config: OpenAIConfig,
    limiter: Limiter,
    cache: Cache | None,
    cache_interactor: CacheInteractor | None,
    events: LLMEvents | None,
) -> OpenAITextChatLLM:
    operation = "chat"
    result = OpenAITextChatLLMImpl(
        client,
        model=config.model,
        model_parameters=config.chat_parameters,
        cache=cache_interactor or CacheInteractor(events, cache),
        events=events,
        json_handler=create_json_handler(config.json_strategy, config.max_json_retries),
        usage_extractor=OpenAIUsageExtractor(),
        history_extractor=OpenAIHistoryExtractor(),
        variable_injector=VariableInjector(),
        retryer=create_retryer(config=config, operation=operation, events=events),
        rate_limiter=create_rate_limiter(config=config, limiter=limiter, events=events),
    )

    return OpenAIParseToolsLLM(result)


def _create_openai_streaming_chat_llm(
    *,
    client: OpenAIClient,
    config: OpenAIConfig,
    limiter: Limiter,
    events: LLMEvents | None,
) -> OpenAIStreamingChatLLM:
    """Create an OpenAI streaming chat LLM."""
    return OpenAIStreamingChatLLMImpl(
        client,
        model=config.model,
        model_parameters=config.chat_parameters,
        events=events,
        emit_usage=config.track_stream_usage,
        variable_injector=VariableInjector(),
        rate_limiter=create_rate_limiter(limiter=limiter, config=config, events=events),
    )

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/openai/factories/embeddings.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""Factory functions for creating OpenAI LLMs."""

from fnllm.caching.base import Cache
from fnllm.events.base import LLMEvents
from fnllm.openai.config import OpenAIConfig
from fnllm.openai.llm.embeddings import OpenAIEmbeddingsLLMImpl
from fnllm.openai.llm.services.usage_extractor import OpenAIUsageExtractor
from fnllm.openai.types.client import OpenAIClient, OpenAIEmbeddingsLLM
from fnllm.services.cache_interactor import CacheInteractor
from fnllm.services.variable_injector import VariableInjector

from .client import create_openai_client
from .utils import create_limiter, create_rate_limiter, create_retryer


def create_openai_embeddings_llm(
    config: OpenAIConfig,
    *,
    client: OpenAIClient | None = None,
    cache: Cache | None = None,
    cache_interactor: CacheInteractor | None = None,
    events: LLMEvents | None = None,
) -> OpenAIEmbeddingsLLM:
    """Create an OpenAI embeddings LLM."""
    operation = "embedding"

    if client is None:
        client = create_openai_client(config)

    limiter = create_limiter(config)
    return OpenAIEmbeddingsLLMImpl(
        client,
        model=config.model,
        model_parameters=config.embeddings_parameters,
        cache=cache_interactor or CacheInteractor(events, cache),
        events=events,
        usage_extractor=OpenAIUsageExtractor(),
        variable_injector=VariableInjector(),
        rate_limiter=create_rate_limiter(config=config, events=events, limiter=limiter),
        retryer=create_retryer(config=config, operation=operation, events=events),
    )

================== /Users/abdelmajidboubrik/Library/DataScienceStudio/dss_home/code-envs/python/graphrag/lib/python3.10/site-packages/fnllm/openai/factories/utils.py ==================
# Copyright (c) 2024 Microsoft Corporation.

"""Helper functions for creating OpenAI LLMs."""

from typing import Any

import tiktoken

from fnllm.events.base import LLMEvents
from fnllm.limiting.base import Limiter
from fnllm.limiting.composite import CompositeLimiter
from fnllm.limiting.concurrency import ConcurrencyLimiter
from fnllm.limiting.rpm import RPMLimiter
from fnllm.limiting.tpm import TPMLimiter
from fnllm.openai.config import OpenAIConfig
from fnllm.openai.llm.services.rate_limiter import OpenAIRateLimiter
from fnllm.openai.llm.services.retryer import OpenAIRetryer
from fnllm.services.rate_limiter import RateLimiter
from fnllm.services.retryer import Retryer


def _get_encoding(encoding_name: str) -> tiktoken.Encoding:
    return tiktoken.get_encoding(encoding_name)


def create_limiter(config: OpenAIConfig) -> Limiter:
    """Create an LLM limiter based on the incoming configuration."""
    limiters = []

    if config.max_concurrency:
        limiters.append(ConcurrencyLimiter.from_max_concurrency(config.max_concurrency))

    if config.requests_per_minute:
        limiters.append(
            RPMLimiter.from_rpm(
                config.requests_per_minute, burst_mode=config.requests_burst_mode
            )
        )

    if config.tokens_per_minute:
        limiters.append(TPMLimiter.from_tpm(config.tokens_per_minute))

    return CompositeLimiter(limiters)


def create_rate_limiter(
    *,
    limiter: Limiter,
    config: OpenAIConfig,
    events: LLMEvents | None,
) -> RateLimiter[Any, Any, Any, Any]:
    """Wraps the LLM to be rate limited."""
    return OpenAIRateLimiter(
        encoder=_get_encoding(config.encoding),
        limiter=limiter,
        events=events,
    )


def create_retryer(
    *,
    config: OpenAIConfig,
    operation: str,
    events: LLMEvents | None,
) -> Retryer[Any, Any, Any, Any]:
    """Wraps the LLM with retry logic."""
    return OpenAIRetryer(
        tag=operation,
        max_retries=config.max_retries,
        max_retry_wait=config.max_retry_wait,
        sleep_on_rate_limit_recommendation=config.sleep_on_rate_limit_recommendation,
        events=events,
    )

